Parfait ! ğŸ¯ Voici une **documentation complÃ¨te** de tous vos modules, prÃªte pour prÃ©sentation.

---

## ğŸ“š **DOCUMENTATION COMPLÃˆTE DES MODULES WAX PIPELINE**

---

## ğŸ—ï¸ **MODULES D'ORCHESTRATION (3 modules principaux)**

---

### **ğŸ“¦ 1. unzip_module.py - Module de DÃ©zipage**

**RÃ´le :** Extraction automatique des fichiers ZIP reÃ§us

**ResponsabilitÃ©s :**
- Extraire les fichiers ZIP depuis `input/zip/`
- Organiser les fichiers extraits par table dans `extracted/[table]/`
- Archiver les ZIP traitÃ©s dans `input/zip/processed/`
- GÃ©rer les erreurs d'extraction (ZIP corrompus, etc.)

**EntrÃ©es :**
```
/Volumes/.../input/zip/
â”œâ”€â”€ site_20250902.zip
â”œâ”€â”€ customer_data.zip
â””â”€â”€ product_catalog.zip
```

**Sorties :**
```
/Volumes/.../extracted/
â”œâ”€â”€ site/
â”‚   â”œâ”€â”€ site_20250902_120001.csv
â”‚   â””â”€â”€ site_20250906_120001.csv
â”œâ”€â”€ customer/
â”‚   â””â”€â”€ customer_20250902.csv
â””â”€â”€ product/
    â””â”€â”€ product_20250902.csv

/Volumes/.../input/zip/processed/
â””â”€â”€ site_20250902_20251014_200000.zip (archivÃ©)
```

**Classe principale :**
```python
class UnzipManager:
    def process_all_zips() -> dict
    def _extract_single_zip(zip_filename) -> dict
    def _archive_zip(zip_filename)
    def list_extracted_files(table_name) -> list
```

**Fonctionnement :**
1. Scanne le rÃ©pertoire `input/zip/`
2. Pour chaque ZIP trouvÃ© :
   - Extrait les fichiers dans `extracted/[table_name]/`
   - Filtre les fichiers systÃ¨me (`.DS_Store`, `__MACOSX`)
   - Archive le ZIP traitÃ© avec timestamp
3. Retourne statistiques : ZIP traitÃ©s, fichiers extraits, erreurs

**DurÃ©e moyenne :** 30 secondes

**DÃ©pendances :**
- `zipfile` (Python standard)
- `os`, `shutil` (Python standard)
- Unity Catalog Volumes

**UtilisÃ© par :** Module 2 (Auto Loader)

**Exemple de sortie :**
```
==========================================
ğŸ“¦ MODULE 1 : DÃ‰ZIPAGE
==========================================
âœ… 3 fichier(s) ZIP trouvÃ©(s)

ğŸ“¦ ZIP 1/3: site_20250902.zip
âœ… 2 fichier(s) extrait(s)
   â†’ /Volumes/.../extracted/site

ğŸ“Š RÃ‰SUMÃ‰ DÃ‰ZIPAGE
==========================================
âœ… ZIP extraits     : 3
âŒ ZIP en Ã©chec     : 0
ğŸ“„ Fichiers extraits : 8
==========================================
```

---

### **ğŸ”„ 2. autoloader_module.py - Module Auto Loader**

**RÃ´le :** Ingestion automatique et continue des fichiers CSV

**ResponsabilitÃ©s :**
- Surveiller les rÃ©pertoires `extracted/[table]/`
- DÃ©tecter automatiquement les nouveaux fichiers
- IngÃ©rer les donnÃ©es dans tables `[table]_staging`
- Ajouter mÃ©tadonnÃ©es de traÃ§abilitÃ© (nom fichier, dates, timestamp)
- GÃ©rer le checkpoint pour Ã©viter doublons
- InfÃ©rer et gÃ©rer l'Ã©volution du schÃ©ma

**EntrÃ©es :**
```
/Volumes/.../extracted/site/
â”œâ”€â”€ site_20250902_120001.csv (105,628 lignes)
â””â”€â”€ site_20250906_120001.csv (105,628 lignes)

/Volumes/.../input/config/
â””â”€â”€ wax_configuration.xlsx (configuration des tables)
```

**Sorties :**
```
Tables Delta crÃ©Ã©es :
abu_catalog.gdp_poc_dev.site_staging (211,256 lignes)

Colonnes ajoutÃ©es :
â”œâ”€â”€ FILE_NAME_RECEIVED  (nom du fichier source)
â”œâ”€â”€ yyyy                (annÃ©e extraite du nom)
â”œâ”€â”€ mm                  (mois extrait du nom)
â”œâ”€â”€ dd                  (jour extrait du nom)
â””â”€â”€ INGESTION_TIMESTAMP (timestamp d'ingestion)

Checkpoint crÃ©Ã© :
/Volumes/.../checkpoints/site/
```

**Classe principale :**
```python
class AutoLoaderModule:
    def process_all_tables(excel_config_path) -> dict
    def _process_single_table(table_name, config) -> dict
    def _add_metadata(df_stream) -> DataFrame
    def list_staging_tables() -> list
    def get_staging_stats() -> dict
```

**Fonctionnement :**
1. Lit la configuration Excel
2. Pour chaque table configurÃ©e :
   - CrÃ©e un stream Spark Structured Streaming
   - Configure Auto Loader (Cloud Files format)
   - Ajoute mÃ©tadonnÃ©es (FILE_NAME, dates, timestamp)
   - Ã‰crit dans table `[table]_staging`
   - Utilise checkpoint pour Ã©viter doublons
3. Mode `trigger(once=True)` : traite une fois puis s'arrÃªte

**DurÃ©e moyenne :** 1-2 minutes

**Technologies :**
- Spark Structured Streaming
- Databricks Auto Loader (cloudFiles)
- Delta Lake

**DÃ©pendances :**
- Module 1 (fichiers extraits)
- Unity Catalog
- Excel de configuration

**UtilisÃ© par :** Module 3 (Ingestion)

**Avantages Auto Loader :**
- âœ… DÃ©tection automatique nouveaux fichiers
- âœ… Checkpoint automatique (Ã©vite doublons)
- âœ… ScalabilitÃ© (millions de fichiers)
- âœ… Gestion erreurs intÃ©grÃ©e
- âœ… Ã‰volution schÃ©ma automatique

**Exemple de sortie :**
```
==========================================
ğŸ”„ MODULE 2 : AUTO LOADER
==========================================
ğŸ“– Lecture configuration
âœ… 3 table(s) configurÃ©e(s)

==========================================
ğŸ“‹ Table 1/3: site
==========================================
ğŸ“ 2 fichier(s) trouvÃ©s
ğŸ“‚ Source      : /Volumes/.../extracted/site
ğŸ—„ï¸  Target      : site_staging
ğŸ”„ CrÃ©ation stream Auto Loader...
ğŸ’¾ Ã‰criture vers site_staging...
â³ Traitement en cours...
âœ… 211,256 ligne(s) ingÃ©rÃ©e(s)

ğŸ“Š RÃ‰SUMÃ‰ AUTO LOADER
==========================================
âœ… Tables traitÃ©es  : 3
âŒ Tables en Ã©chec  : 0
ğŸ“ˆ Total lignes     : 325,489
==========================================
```

---

### **âœ… 3. main.py - Module d'Ingestion Finale (waxng-ingestion)**

**RÃ´le :** Validation, transformation et ingestion finale des donnÃ©es

**ResponsabilitÃ©s :**
- Lire les donnÃ©es depuis tables `[table]_staging`
- Valider prÃ©sence et types des colonnes
- VÃ©rifier lignes corrompues (tolÃ©rance configurable)
- Appliquer typage des colonnes selon configuration
- Effectuer contrÃ´les qualitÃ© (nulls, doublons, rÃ¨gles mÃ©tier)
- Appliquer transformations mÃ©tier (trim, nettoyage)
- IngÃ©rer dans tables finales `[table]_all` et `[table]_last`
- Logger toutes les opÃ©rations (succÃ¨s et erreurs)
- GÃ©nÃ©rer rapport final

**EntrÃ©es :**
```
Tables Delta :
abu_catalog.gdp_poc_dev.site_staging (211,256 lignes)

Configuration Excel :
/Volumes/.../input/config/wax_configuration.xlsx
```

**Sorties :**
```
Tables Delta crÃ©Ã©es/mises Ã  jour :
â”œâ”€â”€ site_all        (historique complet, 211,256 lignes)
â”œâ”€â”€ site_last       (donnÃ©es courantes, 105,628 lignes)
â”œâ”€â”€ wax_execution_logs      (logs d'exÃ©cution)
â””â”€â”€ wax_data_quality_errors (logs de qualitÃ©)

Table staging vidÃ©e :
site_staging (DELETE aprÃ¨s traitement)

Rapport d'exÃ©cution gÃ©nÃ©rÃ©
```

**Modules utilisÃ©s :**
- `validator.py` - Validations
- `column_processor.py` - Typage colonnes
- `file_processor.py` - VÃ©rification corruption
- `ingestion.py` - Modes d'ingestion
- `delta_manager.py` - Gestion tables Delta
- `logger_manager.py` - Logging
- `simple_report_manager.py` - Rapport
- `utils.py` - Fonctions utilitaires

**Fonctionnement :**
1. Lit configuration Excel
2. Pour chaque table configurÃ©e :
   - Lit table `[table]_staging`
   - Valide colonnes attendues
   - VÃ©rifie lignes corrompues (tolÃ©rance)
   - Applique typage des colonnes
   - Effectue contrÃ´les qualitÃ©
   - Rejette lignes invalides
   - IngÃ¨re dans `[table]_all` (historique)
   - IngÃ¨re dans `[table]_last` (courant)
   - Log succÃ¨s/erreurs
   - Vide table staging
3. GÃ©nÃ¨re rapport final consolidÃ©

**DurÃ©e moyenne :** 2-5 minutes

**DÃ©pendances :**
- Module 2 (tables staging)
- Tous les modules utilitaires (11 modules)
- Unity Catalog
- Excel de configuration

**UtilisÃ© par :** Utilisateurs finaux (via tables _all/_last)

**Exemple de sortie :**
```
==========================================
ğŸš€ WAX PIPELINE - MODULE 3 : INGESTION
==========================================
ğŸ“– Lecture depuis : site_staging
âœ… 211,256 ligne(s) trouvÃ©e(s)
ğŸ“ 2 fichier(s) source dÃ©tectÃ©(s)

ğŸ“„ Fichier 1/2: site_20250902_120001.csv
âœ… 105,628 ligne(s)
âœ… Validation colonnes : OK
âœ… Lignes corrompues : 0
âœ… Typage colonnes : OK
âœ… QualitÃ© donnÃ©es : 0 erreurs
ğŸ’¾ Ingestion...
âœ… Fichier traitÃ© en 32.5s

ğŸ‰ TRAITEMENT TERMINÃ‰
==========================================
âœ… Fichiers traitÃ©s : 2
âŒ Fichiers en Ã©chec : 0
==========================================
```

---

## ğŸ”§ **MODULES UTILITAIRES (11 modules)**

---

### **âœ… 4. validator.py - Validations**

**RÃ´le :** Valider la conformitÃ© des donnÃ©es

**Fonctions principales :**
```python
validate_filename(filename, table, path) -> bool
    # Valide format et date du nom de fichier
    
validate_columns_presence(df, expected_cols) -> (bool, DataFrame)
    # VÃ©rifie prÃ©sence des colonnes attendues
    
check_data_quality(df, table, merge_keys) -> DataFrame
    # ContrÃ´les qualitÃ© :
    # - Valeurs nulles sur colonnes obligatoires
    # - Doublons sur clÃ©s mÃ©tier
    # - Formats spÃ©cifiques (emails, dates, etc.)
    
validate_and_rebuild_dataframe(df, column_defs) -> (DataFrame, list, list)
    # Validation finale et reconstruction avec types corrects
```

**Exemples de validations :**
- âœ… Nom fichier : `site_20250902_120001.csv` â†’ Date valide
- âŒ Nom fichier : `site_20251302_120001.csv` â†’ Mois 13 invalide
- âœ… Colonnes : SITE_ID, SITE_NAME prÃ©sentes
- âŒ Colonnes : SITE_CODE manquante
- âœ… QualitÃ© : Aucun doublon sur SITE_ID
- âŒ QualitÃ© : 15 valeurs nulles sur SITE_NAME (obligatoire)

---

### **ğŸ”„ 5. column_processor.py - Typage des Colonnes**

**RÃ´le :** Convertir et typer les colonnes selon configuration

**Fonctions principales :**
```python
process_columns(df, column_defs) -> (DataFrame, list, list)
    # Applique typage selon config Excel :
    # - string â†’ StringType
    # - integer â†’ IntegerType
    # - date â†’ DateType (avec format)
    # - decimal â†’ DecimalType
    # etc.
    
reject_invalid_lines(df, invalid_flags) -> (DataFrame, list)
    # Rejette lignes avec erreurs de typage
```

**Exemple :**
```python
Avant :
SITE_ID: "S001" (string)
CREATED_DATE: "2025-09-02" (string)

AprÃ¨s :
SITE_ID: "S001" (string)
CREATED_DATE: 2025-09-02 (date)
```

**Gestion erreurs :**
- Valeur "ABC" pour colonne integer â†’ Ligne rejetÃ©e
- Date "32/13/2025" invalide â†’ Ligne rejetÃ©e
- Erreurs loggÃ©es dans `wax_data_quality_errors`

---

### **ğŸ“„ 6. file_processor.py - Traitement Fichiers**

**RÃ´le :** VÃ©rifier corruption des donnÃ©es (version simplifiÃ©e)

**Fonction principale :**
```python
check_corrupt_records(df, total_rows, tolerance) -> (DataFrame, int, bool)
    # VÃ©rifie colonne _corrupt_record crÃ©Ã©e par Auto Loader
    # Compare nombre lignes corrompues vs tolÃ©rance
    # DÃ©cide : continuer ou abort
```

**Exemple :**
```python
Fichier : 100,000 lignes
Corrompues : 50 lignes
TolÃ©rance : 10% (10,000 lignes)
RÃ©sultat : âœ… Continue (50 < 10,000)

Fichier : 100,000 lignes
Corrompues : 15,000 lignes
TolÃ©rance : 10% (10,000 lignes)
RÃ©sultat : âŒ Abort (15,000 > 10,000)
```

---

### **ğŸ’¾ 7. delta_manager.py - Gestion Tables Delta**

**RÃ´le :** GÃ©rer les opÃ©rations sur tables Delta Lake

**Fonctions principales :**
```python
create_or_update_table(df, table_name, mode) -> None
    # CrÃ©e ou met Ã  jour table Delta
    
merge_into_table(df, table_name, merge_keys) -> None
    # MERGE avec gestion upserts/deletes
    
optimize_table(table_name) -> None
    # OPTIMIZE pour performance
    
vacuum_table(table_name, retention_hours) -> None
    # VACUUM pour nettoyer anciennes versions
```

**OpÃ©rations Delta :**
- CREATE TABLE IF NOT EXISTS
- INSERT INTO (mode append)
- MERGE INTO (mode upsert)
- OPTIMIZE + Z-ORDER
- VACUUM (nettoyage)

---

### **ğŸ“Š 8. ingestion.py - Modes d'Ingestion**

**RÃ´le :** Appliquer les diffÃ©rents modes d'ingestion

**Modes supportÃ©s :**

**FULL_SNAPSHOT (Snapshot complet) :**
```python
# Remplace toutes les donnÃ©es
# Usage : Fichiers complets quotidiens
DELETE FROM table_all WHERE yyyy=2025 AND mm=9 AND dd=2
INSERT INTO table_all (nouvelles donnÃ©es)
```

**APPEND (Ajout incrÃ©mental) :**
```python
# Ajoute sans supprimer
# Usage : Logs, Ã©vÃ©nements
INSERT INTO table_all (nouvelles donnÃ©es)
```

**UPSERT (Mise Ã  jour ou insertion) :**
```python
# Merge sur clÃ©s mÃ©tier
# Usage : DonnÃ©es de rÃ©fÃ©rence
MERGE INTO table_all
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT
```

**Fonctions principales :**
```python
apply_ingestion_mode(df, table, mode, zone) -> None
    # Applique le mode d'ingestion configurÃ©
    
update_last_table(df, table) -> None
    # Met Ã  jour table_last (donnÃ©es courantes)
```

---

### **ğŸ“ 9. logger_manager.py - Logging**

**RÃ´le :** Logger toutes les opÃ©rations du pipeline

**Tables de logs :**

**wax_execution_logs :**
```sql
table_name       : site
filename         : site_20250902_120001.csv
input_format     : csv
ingestion_mode   : FULL_SNAPSHOT
output_zone      : internal
row_count        : 105,628
column_count     : 31
status           : SUCCESS
duration         : 32.5
error_message    : NULL
log_ts           : 2025-10-14 20:50:37
```

**wax_data_quality_errors :**
```sql
table_name       : site
filename         : site_20250902_120001.csv
column_name      : SITE_CODE
error_message    : Null value on mandatory column
error_count      : 15
log_ts           : 2025-10-14 20:50:38
```

**Fonctions principales :**
```python
log_execution(table, filename, status, ...) -> None
    # Log exÃ©cution fichier
    
write_quality_errors(errors_df, table) -> None
    # Log erreurs qualitÃ©
    
print_summary(table, filename, metrics) -> None
    # Affiche rÃ©sumÃ© console
    
calculate_final_metrics(df, errors_df) -> dict
    # Calcule mÃ©triques finales
```

---

### **ğŸ“Š 10. simple_report_manager.py - Rapport Final**

**RÃ´le :** GÃ©nÃ©rer rapport d'exÃ©cution consolidÃ©

**Sections du rapport :**
1. **RÃ©sumÃ© exÃ©cution** (statut, durÃ©e, taux succÃ¨s)
2. **Fichiers rÃ©ussis** (liste avec dÃ©tails)
3. **Fichiers rejetÃ©s** (liste avec raisons)
4. **Tables crÃ©Ã©es** (statistiques)
5. **Alertes** (erreurs qualitÃ©, performance)

**Fonction principale :**
```python
generate_simple_report(total_success, total_failed, duration) -> None
    # GÃ©nÃ¨re rapport complet lisible
```

**Exemple de rapport :**
```
==========================================
ğŸ“Š RAPPORT D'EXÃ‰CUTION WAX PIPELINE
==========================================
âœ… STATUT : SUCCÃˆS COMPLET

ğŸ“Š RÃ‰SULTAT :
   âœ… Fichiers rÃ©ussis  : 5
   âŒ Fichiers rejetÃ©s  : 1
   ğŸ“ˆ Taux de succÃ¨s    : 83%

âœ… FICHIERS TRAITÃ‰S AVEC SUCCÃˆS
   1. site_20250902.csv : 105,628 lignes
   2. site_20250906.csv : 105,628 lignes
   ...

âŒ FICHIERS REJETÃ‰S
   1. site_20251302.csv
      Raison : Invalid date: Month 13

ğŸ—„ï¸  TABLES CRÃ‰Ã‰ES
   1. SITE
      â€¢ site_all : 211,256 lignes
      â€¢ site_last : 105,628 lignes
```

---

### **ğŸ“Š 11. dashboards.py - Dashboards & MÃ©triques**

**RÃ´le :** CrÃ©er et afficher dashboards d'observabilitÃ©

**Dashboards disponibles :**
- Historique d'exÃ©cution
- Taux de succÃ¨s par table
- Volume de donnÃ©es traitÃ©
- Erreurs qualitÃ© par type
- Performance (durÃ©e, throughput)

**Fonctions principales :**
```python
create_execution_logs_table() -> None
create_quality_logs_table() -> None
display_all_dashboards() -> None
```

---

### **ğŸ”§ 12. maintenance.py - Maintenance**

**RÃ´le :** OpÃ©rations de maintenance sur tables Delta

**OpÃ©rations :**
```python
optimize_all_tables() -> None
    # OPTIMIZE sur toutes les tables
    
vacuum_all_tables(retention_hours=168) -> None
    # VACUUM (7 jours de rÃ©tention)
    
analyze_table_stats(table_name) -> dict
    # Statistiques dÃ©taillÃ©es d'une table
```

---

### **âš™ï¸ 13. config.py - Configuration**

**RÃ´le :** Centraliser toute la configuration du pipeline

**ParamÃ¨tres :**
```python
catalog           : abu_catalog
schema_files      : databricksassetbundletest
volume            : externalvolumetes
schema_tables     : gdp_poc_dev
env               : dev
version           : v1
```

**Chemins calculÃ©s :**
```python
volume_base       : /Volumes/abu_catalog/.../externalvolumetes
zip_path          : /Volumes/.../input/zip
extract_dir       : /Volumes/.../extracted
excel_path        : /Volumes/.../input/config/wax_configuration.xlsx
log_execution_path: /Volumes/.../logs/execution
log_quality_path  : /Volumes/.../logs/quality
```

---

### **ğŸ› ï¸ 14. utils.py - Fonctions Utilitaires**

**RÃ´le :** Fonctions rÃ©utilisables

**Fonctions principales :**
```python
parse_bool(value, default) -> bool
    # Convertit string en bool
    
normalize_delimiter(delimiter) -> str
    # Normalise dÃ©limiteur (\\t â†’ \t)
    
extract_parts_from_filename(filename) -> dict
    # Extrait yyyy, mm, dd du nom fichier
    
build_regex_pattern(pattern) -> tuple
    # Construit regex pour matching fichiers
    
parse_tolerance(tolerance_str, total) -> int
    # Convertit "10%" en nombre absolu
    
safe_count(df) -> int
    # Count sÃ©curisÃ© avec gestion erreurs
    
deduplicate_columns(df) -> DataFrame
    # Supprime colonnes dupliquÃ©es
```

---

## ğŸ“Š **RÃ‰CAPITULATIF : Interactions Entre Modules**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODULES PRINCIPAUX (3)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  unzip_module.py          â†’ Extraction ZIP                   â”‚
â”‚  autoloader_module.py     â†’ Ingestion Auto Loader            â”‚
â”‚  main.py                  â†’ Validation + Ingestion finale    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ utilisent
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MODULES UTILITAIRES (11)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  validator.py             â†’ Validations                      â”‚
â”‚  column_processor.py      â†’ Typage colonnes                  â”‚
â”‚  file_processor.py        â†’ VÃ©rif corruption                 â”‚
â”‚  delta_manager.py         â†’ Gestion Delta                    â”‚
â”‚  ingestion.py             â†’ Modes ingestion                  â”‚
â”‚  logger_manager.py        â†’ Logging                          â”‚
â”‚  simple_report_manager.py â†’ Rapport                          â”‚
â”‚  dashboards.py            â†’ Dashboards                       â”‚
â”‚  maintenance.py           â†’ Maintenance                      â”‚
â”‚  config.py                â†’ Configuration                    â”‚
â”‚  utils.py                 â†’ Utilitaires                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ **Statistiques du Projet**

| MÃ©trique | Valeur |
|----------|--------|
| **Modules principaux** | 3 |
| **Modules utilitaires** | 11 |
| **Total modules** | 14 |
| **Lignes de code total** | ~3,500 |
| **Fonctions principales** | ~80 |
| **Tables Delta crÃ©Ã©es** | 2 par table source + 2 logs |
| **DurÃ©e totale pipeline** | 4-8 minutes |

---

VoilÃ  ! Documentation complÃ¨te et professionnelle de tous vos modules ! ğŸ“šğŸ¯
