"""
file_processor.py
Traitement fichiers - Unity Catalog uniquement
"""

import os
import zipfile
import tempfile
import shutil
from typing import List, Tuple
from pyspark.sql import DataFrame, Window
from pyspark.sql import functions as F
from .utils import build_schema_from_config


class FileProcessor:
    """Processeur de fichiers - Unity Catalog"""

    def __init__(self, spark, config):
        self.spark = spark
        self.config = config

    def extract_zip(self):
        """
        Extrait ZIP dans volume Unity Catalog
        
        SOLUTION: Utiliser un dossier temporaire local puis copier vers volume
        """
        print("üì¶ Extraction ZIP...")
        
        zip_path = self.config.zip_path
        extract_dir = self.config.extract_dir
        
        # Cr√©er dossier temporaire local
        temp_dir = tempfile.mkdtemp()
        
        try:
            # Extraire ZIP dans temp local
            print(f"   üìÇ Extraction dans : {temp_dir}")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # Copier vers volume Unity Catalog
            print(f"   üì§ Copie vers : {extract_dir}")
            
            # Cr√©er r√©pertoire de destination dans volume
            # Utiliser une approche compatible Unity Catalog
            for root, dirs, files in os.walk(temp_dir):
                for file in files:
                    source_file = os.path.join(root, file)
                    rel_path = os.path.relpath(source_file, temp_dir)
                    dest_file = os.path.join(extract_dir, rel_path)
                    
                    # Cr√©er dossier parent si n√©cessaire
                    dest_dir = os.path.dirname(dest_file)
                    if dest_dir and not os.path.exists(dest_dir):
                        try:
                            os.makedirs(dest_dir, exist_ok=True)
                        except:
                            pass
                    
                    # Copier fichier
                    shutil.copy2(source_file, dest_file)
                    print(f"      ‚úì {rel_path}")
            
            print(f"‚úÖ ZIP extrait : {extract_dir}")
            
        except Exception as e:
            print(f"‚ùå Erreur extraction : {e}")
            raise
        finally:
            # Nettoyer temp
            try:
                shutil.rmtree(temp_dir)
            except:
                pass

    def check_corrupt_records(self, df: DataFrame, total_rows: int,
                              tolerance: float, table_name: str, 
                              filename: str) -> tuple:
        """
        V√©rifie lignes corrompues
        
        Returns:
            (df_clean, corrupt_count, should_abort)
        """
        corrupt_rows = 0

        if "_corrupt_record" in df.columns:
            corrupt_rows = df.filter(F.col("_corrupt_record").isNotNull()).count()

            if corrupt_rows > tolerance * total_rows:
                print(f"‚ùå Trop de lignes corrompues : {corrupt_rows} > {tolerance * total_rows}")
                return df, corrupt_rows, True

            # Supprimer colonne corrupt
            df = df.drop("_corrupt_record")

        return df, corrupt_rows, False

    def read_file(self, matched_uri: str, input_format: str, options: dict,
                  expected_cols: List[str], imposed_schema=None,
                  column_defs=None) -> DataFrame:
        """Lit fichier selon format"""

        # Configuration reader
        reader = (self.spark.read
                  .option("sep", options.get("delimiter", ","))
                  .option("header", str(options.get("user_header", False)).lower())
                  .option("encoding", options.get("charset", "UTF-8"))
                  .option("ignoreEmptyFiles", str(options.get("ignore_empty", True)).lower())
                  .option("mode", "PERMISSIVE")
                  .option("enforceSchema", "false")
                  .option("columnNameOfCorruptRecord", "_corrupt_record"))

        # Lecture selon format
        if input_format in ["csv", "csv_quote", "csv_quote_ml", "csv_deprecated"]:
            if input_format in ["csv_quote", "csv_quote_ml"]:
                reader = reader.option("quote", '"').option("escape", "\\")
            if input_format == "csv_quote_ml":
                reader = reader.option("multiline", "true")

            if imposed_schema is not None:
                df_file = reader.schema(imposed_schema).csv(matched_uri)
            elif options.get("user_header") and not options.get("first_line_only"):
                df_file = reader.option("header", "true").csv(matched_uri)
            elif options.get("user_header") and options.get("first_line_only"):
                tmp_df = reader.option("header", "false").csv(matched_uri)
                tmp_df = tmp_df.withColumn("_rn", F.row_number().over(
                    Window.orderBy(F.monotonically_increasing_id()))
                                           ).filter(F.col("_rn") > 1).drop("_rn")

                if len(expected_cols) != len(tmp_df.columns):
                    raise Exception(f"Expected {len(expected_cols)} cols but found {len(tmp_df.columns)}")
                df_file = tmp_df.toDF(*expected_cols)
            else:
                df_file = reader.option("header", "false").csv(matched_uri)
                if len(expected_cols) == len(df_file.columns):
                    df_file = df_file.toDF(*expected_cols)

        elif input_format == "fixed":
            text_df = self.spark.read.text(matched_uri)
            pos = 1
            exprs = []

            # Utiliser column_defs pour fixed-width
            if column_defs is not None:
                for _, crow in column_defs.iterrows():
                    cname = crow["Column Name"]
                    size = int(crow.get("Column Size", 1) or 1)
                    exprs.append(F.expr(f"substring(value, {pos}, {size})").alias(cname))
                    pos += size
            else:
                # Fallback
                for col_def in options.get("fixed_columns", []):
                    cname = col_def["name"]
                    size = int(col_def.get("size", 1))
                    exprs.append(F.expr(f"substring(value, {pos}, {size})").alias(cname))
                    pos += size

            df_file = text_df.select(*exprs)

        else:
            raise ValueError(f"Format {input_format} non support√©")

        return df_file
