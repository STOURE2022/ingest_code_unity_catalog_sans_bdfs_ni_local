Parfait ! üéØ Je vais cr√©er un rapport simplifi√© et clair avec seulement les informations essentielles.
üìä Version Simplifi√©e du Rapport
Cr√©ez simple_report_manager.py :
"""
simple_report_manager.py
Rapport simplifi√© avec informations essentielles uniquement
"""

from datetime import datetime
from pyspark.sql import functions as F


class SimpleReportManager:
    """Gestionnaire de rapport simplifi√© et lisible"""
    
    def __init__(self, spark, config):
        self.spark = spark
        self.config = config
        self.execution_start = datetime.now()
    
    def generate_simple_report(self, total_files_processed: int, total_failed: int,
                               execution_time: float):
        """
        G√©n√®re un rapport simple et clair
        
        Args:
            total_files_processed: Nombre de fichiers r√©ussis
            total_failed: Nombre de fichiers √©chou√©s
            execution_time: Dur√©e totale en secondes
        """
        
        print("\n" + "=" * 100)
        print("üìä RAPPORT D'EX√âCUTION WAX PIPELINE")
        print("=" * 100)
        
        # ========== 1. R√âSUM√â EX√âCUTION ==========
        self._print_execution_summary(total_files_processed, total_failed, execution_time)
        
        # ========== 2. FICHIERS R√âUSSIS ==========
        self._print_successful_files()
        
        # ========== 3. FICHIERS REJET√âS ==========
        self._print_rejected_files()
        
        # ========== 4. TABLES CR√â√âES ==========
        self._print_created_tables()
        
        # ========== 5. ALERTES ==========
        self._print_alerts()
        
        print("\n" + "=" * 100)
        print("‚úÖ Rapport termin√©")
        print("=" * 100 + "\n")
    
    def _print_execution_summary(self, total_success: int, total_failed: int, 
                                 execution_time: float):
        """R√©sum√© de l'ex√©cution"""
        
        total_files = total_success + total_failed
        success_rate = (total_success / total_files * 100) if total_files > 0 else 0
        
        # Ic√¥ne selon taux de succ√®s
        if success_rate == 100:
            status_icon = "‚úÖ"
            status_text = "SUCC√àS COMPLET"
        elif success_rate >= 75:
            status_icon = "‚ö†Ô∏è"
            status_text = "SUCC√àS PARTIEL"
        else:
            status_icon = "‚ùå"
            status_text = "√âCHEC PARTIEL"
        
        print(f"""
{status_icon} STATUT : {status_text}

üìÖ Date      : {self.execution_start.strftime('%Y-%m-%d %H:%M:%S')}
‚è±Ô∏è  Dur√©e     : {execution_time:.1f}s ({execution_time/60:.1f} min)
üåç Environnement : {self.config.env.upper()}

üìä R√âSULTAT :
   ‚úÖ Fichiers r√©ussis  : {total_success}
   ‚ùå Fichiers rejet√©s  : {total_failed}
   üìà Taux de succ√®s    : {success_rate:.0f}%
        """)
    
    def _print_successful_files(self):
        """Liste des fichiers trait√©s avec succ√®s"""
        
        print("\n" + "-" * 100)
        print("‚úÖ FICHIERS TRAIT√âS AVEC SUCC√àS")
        print("-" * 100)
        
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            if not self._table_exists(exec_table):
                print("   ‚ÑπÔ∏è  Aucune donn√©e disponible")
                return
            
            # Fichiers r√©ussis du jour
            successful_df = self.spark.table(exec_table).filter(
                (F.to_date(F.col("log_ts")) == F.current_date()) &
                (F.col("status") == "SUCCESS")
            )
            
            count = successful_df.count()
            
            if count == 0:
                print("   ‚ÑπÔ∏è  Aucun fichier trait√© avec succ√®s")
                return
            
            # Afficher les fichiers
            files = successful_df.select(
                F.col("table_name").alias("Table"),
                F.col("filename").alias("Fichier"),
                F.col("row_count").alias("Lignes"),
                F.round(F.col("duration"), 1).alias("Dur√©e (s)"),
                F.date_format(F.col("log_ts"), "HH:mm:ss").alias("Heure")
            ).orderBy("log_ts").collect()
            
            print(f"\n   üìÅ {count} fichier(s) trait√©(s) avec succ√®s:\n")
            
            for idx, file in enumerate(files, 1):
                print(f"   {idx}. {file.Fichier}")
                print(f"      ‚Ä¢ Table      : {file.Table}")
                print(f"      ‚Ä¢ Lignes     : {file.Lignes:,}")
                print(f"      ‚Ä¢ Dur√©e      : {file['Dur√©e (s)']}s")
                print(f"      ‚Ä¢ Heure      : {file.Heure}")
                print()
            
            # Total lignes trait√©es
            total_rows = sum(f.Lignes for f in files)
            print(f"   üìä Total : {total_rows:,} lignes trait√©es avec succ√®s")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur : {e}")
    
    def _print_rejected_files(self):
        """Liste des fichiers rejet√©s avec raisons"""
        
        print("\n" + "-" * 100)
        print("‚ùå FICHIERS REJET√âS")
        print("-" * 100)
        
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            if not self._table_exists(exec_table):
                print("   ‚ÑπÔ∏è  Aucune donn√©e disponible")
                return
            
            # Fichiers √©chou√©s du jour
            failed_df = self.spark.table(exec_table).filter(
                (F.to_date(F.col("log_ts")) == F.current_date()) &
                (F.col("status") == "FAILED")
            )
            
            count = failed_df.count()
            
            if count == 0:
                print("   ‚úÖ Aucun fichier rejet√©")
                return
            
            # Afficher les fichiers rejet√©s
            files = failed_df.select(
                F.col("table_name").alias("Table"),
                F.col("filename").alias("Fichier"),
                F.col("error_message").alias("Raison"),
                F.date_format(F.col("log_ts"), "HH:mm:ss").alias("Heure")
            ).orderBy("log_ts").collect()
            
            print(f"\n   üö´ {count} fichier(s) rejet√©(s):\n")
            
            for idx, file in enumerate(files, 1):
                print(f"   {idx}. {file.Fichier}")
                print(f"      ‚Ä¢ Table      : {file.Table}")
                print(f"      ‚Ä¢ Raison     : {file.Raison}")
                print(f"      ‚Ä¢ Heure      : {file.Heure}")
                print()
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur : {e}")
    
    def _print_created_tables(self):
        """Tables cr√©√©es avec leurs statistiques"""
        
        print("\n" + "-" * 100)
        print("üóÑÔ∏è  TABLES CR√â√âES")
        print("-" * 100)
        
        try:
            # Lister tables WAX
            tables = self.spark.sql(
                f"SHOW TABLES IN {self.config.catalog}.{self.config.schema_tables}"
            ).collect()
            
            wax_tables = [t for t in tables if "_all" in t.tableName or "_last" in t.tableName]
            
            if not wax_tables:
                print("   ‚ÑπÔ∏è  Aucune table cr√©√©e")
                return
            
            # Grouper par base (table_all et table_last ensemble)
            table_groups = {}
            for table in wax_tables:
                base_name = table.tableName.replace("_all", "").replace("_last", "")
                if base_name not in table_groups:
                    table_groups[base_name] = []
                table_groups[base_name].append(table.tableName)
            
            print(f"\n   üìä {len(table_groups)} table(s) cr√©√©e(s):\n")
            
            for idx, (base_name, table_list) in enumerate(table_groups.items(), 1):
                print(f"   {idx}. {base_name.upper()}")
                
                for table_name in sorted(table_list):
                    table_full = f"{self.config.catalog}.{self.config.schema_tables}.{table_name}"
                    
                    try:
                        df = self.spark.table(table_full)
                        count = df.count()
                        
                        # R√©cup√©rer nombre de fichiers sources
                        sources_count = 0
                        if "FILE_NAME_RECEIVED" in df.columns:
                            sources_count = df.select("FILE_NAME_RECEIVED").distinct().count()
                        
                        # Type de table
                        if "_all" in table_name:
                            table_type = "Historique"
                        else:
                            table_type = "Courante"
                        
                        print(f"      ‚Ä¢ {table_name}")
                        print(f"        - Type        : {table_type}")
                        print(f"        - Lignes      : {count:,}")
                        if sources_count > 0:
                            print(f"        - Fichiers    : {sources_count}")
                    
                    except Exception as e:
                        print(f"      ‚Ä¢ {table_name} : Erreur lecture")
                
                print()
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur : {e}")
    
    def _print_alerts(self):
        """Alertes et points d'attention"""
        
        print("\n" + "-" * 100)
        print("‚ö†Ô∏è  ALERTES ET POINTS D'ATTENTION")
        print("-" * 100)
        
        alerts = []
        
        # V√©rifier erreurs qualit√©
        try:
            quality_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_data_quality_errors"
            
            if self._table_exists(quality_table):
                errors_df = self.spark.table(quality_table).filter(
                    F.to_date(F.col("log_ts")) == F.current_date()
                )
                
                error_count = errors_df.count()
                
                if error_count > 0:
                    # Top 3 erreurs
                    top_errors = errors_df.groupBy("error_message").agg(
                        F.sum(F.col("error_count").cast("bigint")).alias("total")
                    ).orderBy(F.desc("total")).limit(3).collect()
                    
                    alerts.append({
                        "type": "QUALIT√â",
                        "severity": "warning" if error_count < 100 else "critical",
                        "message": f"{error_count} erreur(s) de qualit√© d√©tect√©e(s)",
                        "details": [f"{row.error_message}: {row.total}" for row in top_errors]
                    })
        except:
            pass
        
        # V√©rifier performance
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            if self._table_exists(exec_table):
                slow_files = self.spark.table(exec_table).filter(
                    (F.to_date(F.col("log_ts")) == F.current_date()) &
                    (F.col("status") == "SUCCESS") &
                    (F.col("duration") > 60)  # Plus de 60 secondes
                )
                
                slow_count = slow_files.count()
                
                if slow_count > 0:
                    alerts.append({
                        "type": "PERFORMANCE",
                        "severity": "info",
                        "message": f"{slow_count} fichier(s) lent(s) (>60s)",
                        "details": []
                    })
        except:
            pass
        
        # Afficher alertes
        if not alerts:
            print("\n   ‚úÖ Aucune alerte - Ex√©cution parfaite !")
            return
        
        print(f"\n   üìã {len(alerts)} alerte(s) d√©tect√©e(s):\n")
        
        for idx, alert in enumerate(alerts, 1):
            # Ic√¥ne selon s√©v√©rit√©
            if alert["severity"] == "critical":
                icon = "üî¥"
            elif alert["severity"] == "warning":
                icon = "üü†"
            else:
                icon = "üîµ"
            
            print(f"   {idx}. {icon} [{alert['type']}] {alert['message']}")
            
            for detail in alert["details"]:
                print(f"      ‚Ä¢ {detail}")
            print()
    
    def _table_exists(self, table_name: str) -> bool:
        """V√©rifie si une table existe"""
        try:
            self.spark.table(table_name)
            return True
        except:
            return False
üìù Simplification de print_summary() dans logger_manager.py
Remplacez aussi la fonction print_summary() par une version minimaliste :
def print_summary(self, table_name: str, filename: str, total_rows,
                  corrupt_rows: int, anomalies_total: int, cleaned_rows: int,
                  errors_df: DataFrame):
    """R√©sum√© minimaliste par fichier"""
    
    # Une ligne par fichier
    if isinstance(total_rows, tuple):
        rows_str = f"{total_rows[1]:,}"
    else:
        rows_str = f"{total_rows:,}"
    
    status_icon = "‚úÖ" if anomalies_total == 0 else "‚ö†Ô∏è"
    
    print(f"{status_icon} {filename:<45} | Lignes: {rows_str:>10} | Erreurs: {anomalies_total:>4}")
üîß Int√©gration dans main.py
Remplacez la section rapport √† la fin de main.py :
def main():
    """Point d'entr√©e pipeline"""
    
    start_total_time = time.time()  # Au d√©but
    
    # ... tout le code existant ...
    
    # ========== R√âSUM√â FINAL SIMPLIFI√â ==========
    
    execution_time = time.time() - start_total_time
    
    # Rapport simplifi√©
    if total_files_processed > 0 or total_failed > 0:
        try:
            from simple_report_manager import SimpleReportManager
            
            report_manager = SimpleReportManager(spark, config)
            report_manager.generate_simple_report(
                total_files_processed=total_files_processed,
                total_failed=total_failed,
                execution_time=execution_time
            )
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur g√©n√©ration rapport : {e}")
            import traceback
            traceback.print_exc()
    
    print("\nüéØ Pipeline termin√© !")


if __name__ == "__main__":
    main()
üìä Exemple de Rapport G√©n√©r√©
====================================================================================================
üìä RAPPORT D'EX√âCUTION WAX PIPELINE
====================================================================================================

‚úÖ STATUT : SUCC√àS PARTIEL

üìÖ Date      : 2025-10-14 15:30:00
‚è±Ô∏è  Dur√©e     : 67.3s (1.1 min)
üåç Environnement : DEV

üìä R√âSULTAT :
   ‚úÖ Fichiers r√©ussis  : 2
   ‚ùå Fichiers rejet√©s  : 1
   üìà Taux de succ√®s    : 67%

----------------------------------------------------------------------------------------------------
‚úÖ FICHIERS TRAIT√âS AVEC SUCC√àS
----------------------------------------------------------------------------------------------------

   üìÅ 2 fichier(s) trait√©(s) avec succ√®s:

   1. site_20250902_120001.csv
      ‚Ä¢ Table      : site
      ‚Ä¢ Lignes     : 105,628
      ‚Ä¢ Dur√©e      : 32.5s
      ‚Ä¢ Heure      : 15:30:00

   2. site_20250906_120001.csv
      ‚Ä¢ Table      : site
      ‚Ä¢ Lignes     : 105,628
      ‚Ä¢ Dur√©e      : 33.2s
      ‚Ä¢ Heure      : 15:30:35

   üìä Total : 211,256 lignes trait√©es avec succ√®s

----------------------------------------------------------------------------------------------------
‚ùå FICHIERS REJET√âS
----------------------------------------------------------------------------------------------------

   üö´ 1 fichier(s) rejet√©(s):

   1. site_20251302_120001.csv
      ‚Ä¢ Table      : site
      ‚Ä¢ Raison     : Invalid date: Month 13
      ‚Ä¢ Heure      : 15:31:00

----------------------------------------------------------------------------------------------------
üóÑÔ∏è  TABLES CR√â√âES
----------------------------------------------------------------------------------------------------

   üìä 1 table(s) cr√©√©e(s):

   1. SITE
      ‚Ä¢ site_all
        - Type        : Historique
        - Lignes      : 211,256
        - Fichiers    : 2
      ‚Ä¢ site_last
        - Type        : Courante
        - Lignes      : 105,628
        - Fichiers    : 1

----------------------------------------------------------------------------------------------------
‚ö†Ô∏è  ALERTES ET POINTS D'ATTENTION
----------------------------------------------------------------------------------------------------

   ‚úÖ Aucune alerte - Ex√©cution parfaite !

====================================================================================================
‚úÖ Rapport termin√©
====================================================================================================
‚úÖ Avantages du Rapport Simplifi√©
Aspect
Avant
Apr√®s
Longueur
~500 lignes
~50 lignes
Sections
7 sections
5 sections essentielles
D√©tails
Tr√®s technique
Informations cl√©s
Lisibilit√©
Pour d√©veloppeurs
Pour tous
Focus
Tout
R√©ussis + Rejet√©s
Temps lecture
5-10 min
1-2 min
üéØ Ce qui est conserv√© (Essentiel)
‚úÖ Statut global (succ√®s/√©chec)
‚úÖ Dur√©e d'ex√©cution
‚úÖ Liste fichiers r√©ussis avec d√©tails
‚úÖ Liste fichiers rejet√©s avec raisons
‚úÖ Tables cr√©√©es
‚úÖ Alertes importantes
üóëÔ∏è Ce qui est enlev√© (Superflu)
‚ùå Statistiques d√©taill√©es
‚ùå Performance par table
‚ùå Top erreurs exhaustif
‚ùå Recommandations longues
‚ùå M√©triques techniques
Testez et dites-moi si c'est assez clair ! üöÄ
