"""
simple_report_manager.py
Rapport simplifi√© avec informations essentielles uniquement
"""

from datetime import datetime
from pyspark.sql import functions as F


class SimpleReportManager:
    """Gestionnaire de rapport simplifi√© et lisible"""
    
    def __init__(self, spark, config):
        self.spark = spark
        self.config = config
        self.execution_start = datetime.now()
    
    def generate_simple_report(self, total_files_processed: int, total_failed: int,
                               execution_time: float):
        """
        G√©n√®re un rapport simple et clair
        
        Args:
            total_files_processed: Nombre de fichiers r√©ussis
            total_failed: Nombre de fichiers √©chou√©s
            execution_time: Dur√©e totale en secondes
        """
        
        print("\n" + "=" * 100)
        print("üìä RAPPORT D'EX√âCUTION WAX PIPELINE")
        print("=" * 100)
        
        # ========== 1. R√âSUM√â EX√âCUTION ==========
        self._print_execution_summary(total_files_processed, total_failed, execution_time)
        
        # ========== 2. FICHIERS R√âUSSIS ==========
        self._print_successful_files()
        
        # ========== 3. FICHIERS REJET√âS ==========
        self._print_rejected_files()
        
        # ========== 4. TABLES CR√â√âES ==========
        self._print_created_tables()
        
        # ========== 5. ALERTES ==========
        self._print_alerts()
        
        print("\n" + "=" * 100)
        print("‚úÖ Rapport termin√©")
        print("=" * 100 + "\n")
    
    def _print_execution_summary(self, total_success: int, total_failed: int, 
                                 execution_time: float):
        """R√©sum√© de l'ex√©cution"""
        
        total_files = total_success + total_failed
        success_rate = (total_success / total_files * 100) if total_files > 0 else 0
        
        # Ic√¥ne selon taux de succ√®s
        if success_rate == 100:
            status_icon = "‚úÖ"
            status_text = "SUCC√àS COMPLET"
        elif success_rate >= 75:
            status_icon = "‚ö†Ô∏è"
            status_text = "SUCC√àS PARTIEL"
        else:
            status_icon = "‚ùå"
            status_text = "√âCHEC PARTIEL"
        
        print(f"""
{status_icon} STATUT : {status_text}

üìÖ Date      : {self.execution_start.strftime('%Y-%m-%d %H:%M:%S')}
‚è±Ô∏è  Dur√©e     : {execution_time:.1f}s ({execution_time/60:.1f} min)
üåç Environnement : {self.config.env.upper()}

üìä R√âSULTAT :
   ‚úÖ Fichiers r√©ussis  : {total_success}
   ‚ùå Fichiers rejet√©s  : {total_failed}
   üìà Taux de succ√®s    : {success_rate:.0f}%
        """)
    
    def _print_successful_files(self):
        """Liste des fichiers trait√©s avec succ√®s"""
        
        print("\n" + "-" * 100)
        print("‚úÖ FICHIERS TRAIT√âS AVEC SUCC√àS")
        print("-" * 100)
        
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            # V√©rifier si la table existe
            if not self._table_exists(exec_table):
                print("   ‚ÑπÔ∏è  Table execution_logs non disponible (premi√®re ex√©cution ?)")
                return
            
            # Fichiers r√©ussis du jour
            successful_df = self.spark.table(exec_table).filter(
                (F.to_date(F.col("log_ts")) == F.current_date()) &
                (F.col("status") == "SUCCESS")
            )
            
            count = successful_df.count()
            
            if count == 0:
                print("   ‚ÑπÔ∏è  Aucun fichier trait√© avec succ√®s aujourd'hui")
                return
            
            # Afficher les fichiers
            files = successful_df.select(
                F.col("table_name").alias("Table"),
                F.col("filename").alias("Fichier"),
                F.col("row_count").alias("Lignes"),
                F.round(F.col("duration"), 1).alias("Dur√©e (s)"),
                F.date_format(F.col("log_ts"), "HH:mm:ss").alias("Heure")
            ).orderBy("log_ts").collect()
            
            print(f"\n   üìÅ {count} fichier(s) trait√©(s) avec succ√®s:\n")
            
            for idx, file in enumerate(files, 1):
                print(f"   {idx}. {file.Fichier}")
                print(f"      ‚Ä¢ Table      : {file.Table}")
                print(f"      ‚Ä¢ Lignes     : {file.Lignes:,}")
                print(f"      ‚Ä¢ Dur√©e      : {file['Dur√©e (s)']}s")
                print(f"      ‚Ä¢ Heure      : {file.Heure}")
                print()
            
            # Total lignes trait√©es
            total_rows = sum(f.Lignes for f in files)
            print(f"   üìä Total : {total_rows:,} lignes trait√©es avec succ√®s")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur lecture fichiers r√©ussis : {e}")
    
    def _print_rejected_files(self):
        """Liste des fichiers rejet√©s avec raisons (tous types de rejets)"""
        
        print("\n" + "-" * 100)
        print("‚ùå FICHIERS REJET√âS")
        print("-" * 100)
        
        rejected_files = []
        
        # ========== 1. REJETS DANS EXECUTION_LOGS (√©checs de traitement) ==========
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            # V√©rifier si la table existe
            if self._table_exists(exec_table):
                failed_df = self.spark.table(exec_table).filter(
                    (F.to_date(F.col("log_ts")) == F.current_date()) &
                    (F.col("status") == "FAILED")
                )
                
                if failed_df.count() > 0:
                    for row in failed_df.collect():
                        rejected_files.append({
                            "filename": row.filename,
                            "table": row.table_name,
                            "reason": row.error_message if row.error_message else "Unknown error",
                            "time": row.log_ts.strftime("%H:%M:%S") if row.log_ts else "N/A"
                        })
            else:
                print("   ‚ÑπÔ∏è  Table execution_logs non disponible (premi√®re ex√©cution ?)")
        except Exception as e:
            pass  # Ignorer silencieusement les erreurs
        
        # ========== 2. REJETS DANS QUALITY_ERRORS (rejets de validation) ==========
        try:
            quality_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_data_quality_errors"
            
            # V√©rifier si la table existe
            if self._table_exists(quality_table):
                # Chercher les rejets de fichiers (colonne "filename")
                filename_errors = self.spark.table(quality_table).filter(
                    (F.to_date(F.col("log_ts")) == F.current_date()) &
                    (F.col("column_name") == "filename")
                )
                
                if filename_errors.count() > 0:
                    for row in filename_errors.collect():
                        # V√©rifier si pas d√©j√† dans la liste
                        if not any(f["filename"] == row.filename for f in rejected_files):
                            # Construire raison depuis error_message
                            reason = row.error_message if row.error_message else "Validation failed"
                            
                            rejected_files.append({
                                "filename": row.filename,
                                "table": row.table_name,
                                "reason": reason,
                                "time": row.log_ts.strftime("%H:%M:%S") if row.log_ts else "N/A"
                            })
            else:
                print("   ‚ÑπÔ∏è  Table quality_errors non disponible (premi√®re ex√©cution ?)")
        except Exception as e:
            pass  # Ignorer silencieusement les erreurs
        
        # ========== AFFICHAGE ==========
        
        if not rejected_files:
            print("   ‚úÖ Aucun fichier rejet√©")
            return
        
        print(f"\n   üö´ {len(rejected_files)} fichier(s) rejet√©(s):\n")
        
        for idx, file_info in enumerate(rejected_files, 1):
            print(f"   {idx}. {file_info['filename']}")
            print(f"      ‚Ä¢ Table      : {file_info['table']}")
            print(f"      ‚Ä¢ Raison     : {file_info['reason']}")
            print(f"      ‚Ä¢ Heure      : {file_info['time']}")
            print()
    
    def _print_created_tables(self):
        """Tables cr√©√©es avec leurs statistiques"""
        
        print("\n" + "-" * 100)
        print("üóÑÔ∏è  TABLES CR√â√âES")
        print("-" * 100)
        
        try:
            # Lister tables WAX
            tables = self.spark.sql(
                f"SHOW TABLES IN {self.config.catalog}.{self.config.schema_tables}"
            ).collect()
            
            wax_tables = [t for t in tables if "_all" in t.tableName or "_last" in t.tableName]
            
            if not wax_tables:
                print("   ‚ÑπÔ∏è  Aucune table cr√©√©e")
                return
            
            # Grouper par base (table_all et table_last ensemble)
            table_groups = {}
            for table in wax_tables:
                base_name = table.tableName.replace("_all", "").replace("_last", "")
                if base_name not in table_groups:
                    table_groups[base_name] = []
                table_groups[base_name].append(table.tableName)
            
            print(f"\n   üìä {len(table_groups)} table(s) cr√©√©e(s):\n")
            
            for idx, (base_name, table_list) in enumerate(table_groups.items(), 1):
                print(f"   {idx}. {base_name.upper()}")
                
                for table_name in sorted(table_list):
                    table_full = f"{self.config.catalog}.{self.config.schema_tables}.{table_name}"
                    
                    try:
                        df = self.spark.table(table_full)
                        count = df.count()
                        
                        # R√©cup√©rer nombre de fichiers sources
                        sources_count = 0
                        if "FILE_NAME_RECEIVED" in df.columns:
                            sources_count = df.select("FILE_NAME_RECEIVED").distinct().count()
                        
                        # Type de table
                        if "_all" in table_name:
                            table_type = "Historique"
                        else:
                            table_type = "Courante"
                        
                        print(f"      ‚Ä¢ {table_name}")
                        print(f"        - Type        : {table_type}")
                        print(f"        - Lignes      : {count:,}")
                        if sources_count > 0:
                            print(f"        - Fichiers    : {sources_count}")
                    
                    except Exception as e:
                        print(f"      ‚Ä¢ {table_name} : Erreur lecture")
                
                print()
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur lecture tables : {e}")
    
    def _print_alerts(self):
        """Alertes et points d'attention"""
        
        print("\n" + "-" * 100)
        print("‚ö†Ô∏è  ALERTES ET POINTS D'ATTENTION")
        print("-" * 100)
        
        alerts = []
        
        # V√©rifier erreurs qualit√©
        try:
            quality_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_data_quality_errors"
            
            if self._table_exists(quality_table):
                errors_df = self.spark.table(quality_table).filter(
                    F.to_date(F.col("log_ts")) == F.current_date()
                )
                
                error_count = errors_df.count()
                
                if error_count > 0:
                    # Top 3 erreurs
                    top_errors = errors_df.groupBy("error_message").agg(
                        F.sum(F.col("error_count").cast("bigint")).alias("total")
                    ).orderBy(F.desc("total")).limit(3).collect()
                    
                    alerts.append({
                        "type": "QUALIT√â",
                        "severity": "warning" if error_count < 100 else "critical",
                        "message": f"{error_count} erreur(s) de qualit√© d√©tect√©e(s)",
                        "details": [f"{row.error_message}: {row.total}" for row in top_errors]
                    })
        except:
            pass
        
        # V√©rifier performance
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            if self._table_exists(exec_table):
                slow_files = self.spark.table(exec_table).filter(
                    (F.to_date(F.col("log_ts")) == F.current_date()) &
                    (F.col("status") == "SUCCESS") &
                    (F.col("duration") > 60)  # Plus de 60 secondes
                )
                
                slow_count = slow_files.count()
                
                if slow_count > 0:
                    alerts.append({
                        "type": "PERFORMANCE",
                        "severity": "info",
                        "message": f"{slow_count} fichier(s) lent(s) (>60s)",
                        "details": []
                    })
        except:
            pass
        
        # Afficher alertes
        if not alerts:
            print("\n   ‚úÖ Aucune alerte - Ex√©cution parfaite !")
            return
        
        print(f"\n   üìã {len(alerts)} alerte(s) d√©tect√©e(s):\n")
        
        for idx, alert in enumerate(alerts, 1):
            # Ic√¥ne selon s√©v√©rit√©
            if alert["severity"] == "critical":
                icon = "üî¥"
            elif alert["severity"] == "warning":
                icon = "üü†"
            else:
                icon = "üîµ"
            
            print(f"   {idx}. {icon} [{alert['type']}] {alert['message']}")
            
            for detail in alert["details"]:
                print(f"      ‚Ä¢ {detail}")
            print()
    
    def _table_exists(self, table_name: str) -> bool:
        """V√©rifie si une table existe"""
        try:
            self.spark.table(table_name)
            return True
        except:
            return False
