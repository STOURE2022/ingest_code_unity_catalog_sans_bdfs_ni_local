"""
simple_report_manager.py
Rapport simplifiÃ© avec informations essentielles uniquement
"""

from datetime import datetime
from pyspark.sql import functions as F


class SimpleReportManager:
    """Gestionnaire de rapport simplifiÃ© et lisible"""
    
    def __init__(self, spark, config):
        self.spark = spark
        self.config = config
        self.execution_start = datetime.now()
    
    def generate_simple_report(self, total_files_processed: int, total_failed: int,
                               execution_time: float):
        """
        GÃ©nÃ¨re un rapport simple et clair
        
        Args:
            total_files_processed: Nombre de fichiers rÃ©ussis
            total_failed: Nombre de fichiers Ã©chouÃ©s
            execution_time: DurÃ©e totale en secondes
        """
        
        print("\n" + "=" * 100)
        print("ðŸ“Š RAPPORT D'EXÃ‰CUTION WAX PIPELINE")
        print("=" * 100)
        
        # ========== 1. RÃ‰SUMÃ‰ EXÃ‰CUTION ==========
        self._print_execution_summary(total_files_processed, total_failed, execution_time)
        
        # ========== 2. FICHIERS RÃ‰USSIS ==========
        self._print_successful_files()
        
        # ========== 3. FICHIERS REJETÃ‰S ==========
        self._print_rejected_files()
        
        # ========== 4. TABLES CRÃ‰Ã‰ES ==========
        self._print_created_tables()
        
        # ========== 5. ALERTES ==========
        self._print_alerts()
        
        print("\n" + "=" * 100)
        print("âœ… Rapport terminÃ©")
        print("=" * 100 + "\n")
    
    def _print_execution_summary(self, total_success: int, total_failed: int, 
                                 execution_time: float):
        """RÃ©sumÃ© de l'exÃ©cution"""
        
        total_files = total_success + total_failed
        success_rate = (total_success / total_files * 100) if total_files > 0 else 0
        
        # IcÃ´ne selon taux de succÃ¨s
        if success_rate == 100:
            status_icon = "âœ…"
            status_text = "SUCCÃˆS COMPLET"
        elif success_rate >= 75:
            status_icon = "âš ï¸"
            status_text = "SUCCÃˆS PARTIEL"
        else:
            status_icon = "âŒ"
            status_text = "Ã‰CHEC PARTIEL"
        
        print(f"""
{status_icon} STATUT : {status_text}

ðŸ“… Date      : {self.execution_start.strftime('%Y-%m-%d %H:%M:%S')}
â±ï¸  DurÃ©e     : {execution_time:.1f}s ({execution_time/60:.1f} min)
ðŸŒ Environnement : {self.config.env.upper()}

ðŸ“Š RÃ‰SULTAT :
   âœ… Fichiers rÃ©ussis  : {total_success}
   âŒ Fichiers rejetÃ©s  : {total_failed}
   ðŸ“ˆ Taux de succÃ¨s    : {success_rate:.0f}%
        """)
    
    def _print_successful_files(self):
        """Liste des fichiers traitÃ©s avec succÃ¨s"""
        
        print("\n" + "-" * 100)
        print("âœ… FICHIERS TRAITÃ‰S AVEC SUCCÃˆS")
        print("-" * 100)
        
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            # VÃ©rifier si la table existe
            if not self._table_exists(exec_table):
                print("   â„¹ï¸  Table execution_logs non disponible (premiÃ¨re exÃ©cution ?)")
                return
            
            # Fichiers rÃ©ussis du jour
            successful_df = self.spark.table(exec_table).filter(
                (F.to_date(F.col("log_ts")) == F.current_date()) &
                (F.col("status") == "SUCCESS")
            )
            
            count = successful_df.count()
            
            if count == 0:
                print("   â„¹ï¸  Aucun fichier traitÃ© avec succÃ¨s aujourd'hui")
                return
            
            # Afficher les fichiers
            files = successful_df.select(
                F.col("table_name").alias("Table"),
                F.col("filename").alias("Fichier"),
                F.col("row_count").alias("Lignes"),
                F.round(F.col("duration"), 1).alias("DurÃ©e (s)"),
                F.date_format(F.col("log_ts"), "HH:mm:ss").alias("Heure")
            ).orderBy("log_ts").collect()
            
            print(f"\n   ðŸ“ {count} fichier(s) traitÃ©(s) avec succÃ¨s:\n")
            
            for idx, file in enumerate(files, 1):
                print(f"   {idx}. {file.Fichier}")
                print(f"      â€¢ Table      : {file.Table}")
                print(f"      â€¢ Lignes     : {file.Lignes:,}")
                print(f"      â€¢ DurÃ©e      : {file['DurÃ©e (s)']}s")
                print(f"      â€¢ Heure      : {file.Heure}")
                print()
            
            # Total lignes traitÃ©es
            total_rows = sum(f.Lignes for f in files)
            print(f"   ðŸ“Š Total : {total_rows:,} lignes traitÃ©es avec succÃ¨s")
            
        except Exception as e:
            print(f"   âš ï¸  Erreur lecture fichiers rÃ©ussis : {e}")
    
    def _print_rejected_files(self):
        """Liste des fichiers rejetÃ©s avec raisons (tous types de rejets)"""
        
        print("\n" + "-" * 100)
        print("âŒ FICHIERS REJETÃ‰S")
        print("-" * 100)
        
        rejected_files = []
        
        # ========== 1. REJETS DANS EXECUTION_LOGS (Ã©checs de traitement) ==========
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            # VÃ©rifier si la table existe
            if self._table_exists(exec_table):
                failed_df = self.spark.table(exec_table).filter(
                    (F.to_date(F.col("log_ts")) == F.current_date()) &
                    (F.col("status") == "FAILED")
                )
                
                if failed_df.count() > 0:
                    for row in failed_df.collect():
                        rejected_files.append({
                            "filename": row.filename,
                            "table": row.table_name,
                            "reason": row.error_message if row.error_message else "Unknown error",
                            "time": row.log_ts.strftime("%H:%M:%S") if row.log_ts else "N/A"
                        })
            else:
                print("   â„¹ï¸  Table execution_logs non disponible (premiÃ¨re exÃ©cution ?)")
        except Exception as e:
            pass  # Ignorer silencieusement les erreurs
        
        # ========== 2. REJETS DANS QUALITY_ERRORS (rejets de validation) ==========
        try:
            quality_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_data_quality_errors"
            
            # VÃ©rifier si la table existe
            if self._table_exists(quality_table):
                # Chercher les rejets de fichiers (colonne "filename")
                filename_errors = self.spark.table(quality_table).filter(
                    (F.to_date(F.col("log_ts")) == F.current_date()) &
                    (F.col("column_name") == "filename")
                )
                
                if filename_errors.count() > 0:
                    for row in filename_errors.collect():
                        # VÃ©rifier si pas dÃ©jÃ  dans la liste
                        if not any(f["filename"] == row.filename for f in rejected_files):
                            # Construire raison depuis error_message
                            reason = row.error_message if row.error_message else "Validation failed"
                            
                            rejected_files.append({
                                "filename": row.filename,
                                "table": row.table_name,
                                "reason": reason,
                                "time": row.log_ts.strftime("%H:%M:%S") if row.log_ts else "N/A"
                            })
            else:
                print("   â„¹ï¸  Table quality_errors non disponible (premiÃ¨re exÃ©cution ?)")
        except Exception as e:
            pass  # Ignorer silencieusement les erreurs
        
        # ========== AFFICHAGE ==========
        
        if not rejected_files:
            print("   âœ… Aucun fichier rejetÃ©")
            return
        
        print(f"\n   ðŸš« {len(rejected_files)} fichier(s) rejetÃ©(s):\n")
        
        for idx, file_info in enumerate(rejected_files, 1):
            print(f"   {idx}. {file_info['filename']}")
            print(f"      â€¢ Table      : {file_info['table']}")
            print(f"      â€¢ Raison     : {file_info['reason']}")
            print(f"      â€¢ Heure      : {file_info['time']}")
            print()
    
    def _print_created_tables(self):
        """Tables crÃ©Ã©es avec leurs statistiques"""
        
        print("\n" + "-" * 100)
        print("ðŸ—„ï¸  TABLES CRÃ‰Ã‰ES")
        print("-" * 100)
        
        try:
            # Lister tables WAX
            tables = self.spark.sql(
                f"SHOW TABLES IN {self.config.catalog}.{self.config.schema_tables}"
            ).collect()
            
            wax_tables = [t for t in tables if "_all" in t.tableName or "_last" in t.tableName]
            
            if not wax_tables:
                print("   â„¹ï¸  Aucune table crÃ©Ã©e")
                return
            
            # Grouper par base (table_all et table_last ensemble)
            table_groups = {}
            for table in wax_tables:
                base_name = table.tableName.replace("_all", "").replace("_last", "")
                if base_name not in table_groups:
                    table_groups[base_name] = []
                table_groups[base_name].append(table.tableName)
            
            print(f"\n   ðŸ“Š {len(table_groups)} table(s) crÃ©Ã©e(s):\n")
            
            for idx, (base_name, table_list) in enumerate(table_groups.items(), 1):
                print(f"   {idx}. {base_name.upper()}")
                
                for table_name in sorted(table_list):
                    table_full = f"{self.config.catalog}.{self.config.schema_tables}.{table_name}"
                    
                    try:
                        df = self.spark.table(table_full)
                        count = df.count()
                        
                        # RÃ©cupÃ©rer nombre de fichiers sources
                        sources_count = 0
                        if "FILE_NAME_RECEIVED" in df.columns:
                            sources_count = df.select("FILE_NAME_RECEIVED").distinct().count()
                        
                        # Type de table
                        if "_all" in table_name:
                            table_type = "Historique"
                        else:
                            table_type = "Courante"
                        
                        print(f"      â€¢ {table_name}")
                        print(f"        - Type        : {table_type}")
                        print(f"        - Lignes      : {count:,}")
                        if sources_count > 0:
                            print(f"        - Fichiers    : {sources_count}")
                    
                    except Exception as e:
                        print(f"      â€¢ {table_name} : Erreur lecture")
                
                print()
            
        except Exception as e:
            print(f"   âš ï¸  Erreur lecture tables : {e}")
    
    def _print_alerts(self):
        """Alertes et points d'attention"""
        
        print("\n" + "-" * 100)
        print("âš ï¸  ALERTES ET POINTS D'ATTENTION")
        print("-" * 100)
        
        alerts = []
        
        # VÃ©rifier erreurs qualitÃ©
        try:
            quality_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_data_quality_errors"
            
            if self._table_exists(quality_table):
                errors_df = self.spark.table(quality_table).filter(
                    F.to_date(F.col("log_ts")) == F.current_date()
                )
                
                error_count = errors_df.count()
                
                if error_count > 0:
                    # Top 3 erreurs
                    top_errors = errors_df.groupBy("error_message").agg(
                        F.sum(F.col("error_count").cast("bigint")).alias("total")
                    ).orderBy(F.desc("total")).limit(3).collect()
                    
                    alerts.append({
                        "type": "QUALITÃ‰",
                        "severity": "warning" if error_count < 100 else "critical",
                        "message": f"{error_count} erreur(s) de qualitÃ© dÃ©tectÃ©e(s)",
                        "details": [f"{row.error_message}: {row.total}" for row in top_errors]
                    })
        except:
            pass
        
        # VÃ©rifier performance
        try:
            exec_table = f"{self.config.catalog}.{self.config.schema_tables}.wax_execution_logs"
            
            if self._table_exists(exec_table):
                slow_files = self.spark.table(exec_table).filter(
                    (F.to_date(F.col("log_ts")) == F.current_date()) &
                    (F.col("status") == "SUCCESS") &
                    (F.col("duration") > 60)  # Plus de 60 secondes
                )
                
                slow_count = slow_files.count()
                
                if slow_count > 0:
                    alerts.append({
                        "type": "PERFORMANCE",
                        "severity": "info",
                        "message": f"{slow_count} fichier(s) lent(s) (>60s)",
                        "details": []
                    })
        except:
            pass
        
        # Afficher alertes
        if not alerts:
            print("\n   âœ… Aucune alerte - ExÃ©cution parfaite !")
            return
        
        print(f"\n   ðŸ“‹ {len(alerts)} alerte(s) dÃ©tectÃ©e(s):\n")
        
        for idx, alert in enumerate(alerts, 1):
            # IcÃ´ne selon sÃ©vÃ©ritÃ©
            if alert["severity"] == "critical":
                icon = "ðŸ”´"
            elif alert["severity"] == "warning":
                icon = "ðŸŸ "
            else:
                icon = "ðŸ”µ"
            
            print(f"   {idx}. {icon} [{alert['type']}] {alert['message']}")
            
            for detail in alert["details"]:
                print(f"      â€¢ {detail}")
            print()
    
    def _table_exists(self, table_name: str) -> bool:
        """VÃ©rifie si une table existe"""
        try:
            self.spark.table(table_name)
            return True
        except:
            return False
