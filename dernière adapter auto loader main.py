"""
main.py - MODULE 3 : WAX PIPELINE INGESTION
Traitement final : Validation, Transformation et Ingestion
Lit depuis les tables _staging cr√©√©es par Auto Loader (Module 2)
"""

import sys
import os
import time
from functools import reduce

# Import modules WAX (TOUS conserv√©s !)
from config import Config
from validator import DataValidator
from file_processor import FileProcessor  # ‚úÖ Simplifi√© mais conserv√©
from column_processor import ColumnProcessor
from delta_manager import DeltaManager
from logger_manager import LoggerManager
from ingestion import IngestionManager
from dashboards import DashboardManager
from maintenance import MaintenanceManager
from simple_report_manager import SimpleReportManager
from utils import (
    parse_bool, parse_tolerance, safe_count
)

import pandas as pd
from pyspark.sql import SparkSession, functions as F


def main():
    """Point d'entr√©e Module 3 - Traitement des donn√©es staging"""

    # Timer global pour le rapport
    start_total_time = time.time()

    print("=" * 80)
    print("üöÄ WAX PIPELINE - MODULE 3 : INGESTION")
    print("   (Lecture depuis tables staging)")
    print("=" * 80)

    # ========== CONFIGURATION ==========

    print("\n‚öôÔ∏è  Configuration Unity Catalog...")

    config = Config(
        catalog="abu_catalog",
        schema_files="databricksassetbundletest",
        volume="externalvolumetes",
        schema_tables="gdp_poc_dev",
        env="dev",
        version="v1",
        auto_detect_files=False  # ‚ùå Plus utilis√© (c'√©tait pour le ZIP)
    )

    config.print_config()

    # Validation
    validation = config.validate_paths()
    if not validation["valid"]:
        print("\n‚ö†Ô∏è Avertissements:")
        for issue in validation["issues"]:
            print(f"  {issue}")
    else:
        print(f"\n‚úÖ Configuration valid√©e - {validation['mode']}")
        for issue in validation["issues"]:
            if issue.startswith("‚ÑπÔ∏è"):
                print(f"  {issue}")

    # ========== INITIALISATION SPARK ==========

    print("\n‚ö° Initialisation Spark...")
    spark = SparkSession.builder.getOrCreate()
    print(f"‚úÖ Spark version : {spark.version}")

    # ========== MANAGERS (TOUS CONSERV√âS !) ==========

    validator = DataValidator(spark)
    file_processor = FileProcessor(spark, config)  # ‚úÖ Version simplifi√©e
    column_processor = ColumnProcessor(spark, config)
    delta_manager = DeltaManager(spark, config)
    logger_manager = LoggerManager(spark, config)
    ingestion_manager = IngestionManager(spark, config, delta_manager)
    dashboard_manager = DashboardManager(spark, config)
    maintenance_manager = MaintenanceManager(spark, config)

    print("‚úÖ Managers initialis√©s\n")

    # ========== INITIALISATION TABLES DE LOGS ==========

    print("üîß Initialisation tables de logs...")
    try:
        dashboard_manager.create_execution_logs_table()
        dashboard_manager.create_quality_logs_table()
        print("‚úÖ Tables de logs pr√™tes\n")
    except Exception as e:
        print(f"‚ö†Ô∏è Avertissement tables logs : {e}\n")

    # ‚ùå SUPPRIM√â : EXTRACTION ZIP
    # Cette section est maintenant dans unzip_module.py (Module 1)
    # Les fichiers ont d√©j√† √©t√© extraits par le Module 1

    # ========== LECTURE EXCEL ==========

    print("=" * 80)
    print("üìë LECTURE CONFIGURATION EXCEL")
    print("=" * 80)

    # ‚ú® MODIFI√â : Chemin Excel depuis input/config/
    excel_path = f"{config.volume_base}/input/config/wax_configuration.xlsx"
    print(f"üìÑ Fichier : {excel_path}")

    try:
        file_columns_df = pd.read_excel(
            excel_path.replace("/Volumes", "/dbfs/Volumes"),
            sheet_name="Field-Column"
        )
        file_tables_df = pd.read_excel(
            excel_path.replace("/Volumes", "/dbfs/Volumes"),
            sheet_name="File-Table"
        )
        print(f"‚úÖ Config charg√©e : {len(file_tables_df)} tables, {len(file_columns_df)} colonnes\n")
    except Exception as e:
        print(f"‚ùå Erreur lecture Excel : {e}")
        import traceback
        traceback.print_exc()
        return

    # ========== TRAITEMENT TABLES STAGING ==========

    print("=" * 80)
    print("üìä TRAITEMENT DES TABLES STAGING")
    print("=" * 80)

    total_success = 0
    total_failed = 0
    total_files_processed = 0

    for table_idx, trow in file_tables_df.iterrows():
        start_table_time = time.time()

        source_table = trow["Delta Table Name"]
        output_zone = str(trow.get("Output Zone", "internal")).strip().lower()
        ingestion_mode = str(trow.get("Ingestion mode", "")).strip()

        print(f"\n{'=' * 80}")
        print(f"üìã Table {table_idx + 1}/{len(file_tables_df)}: {source_table}")
        print(f"   Zone: {output_zone}, Mode: {ingestion_mode}")
        print(f"{'=' * 80}")

        # Options (conserv√©es pour les transformations)
        trim_flag = parse_bool(trow.get("Trim", True), True)

        # ========== ‚ú® NOUVEAU : LECTURE DEPUIS TABLE STAGING ==========

        staging_table = f"{config.catalog}.{config.schema_tables}.{source_table}_staging"
        
        print(f"\nüìñ Lecture depuis table staging : {staging_table}")

        try:
            df_staging = spark.table(staging_table)
            
            # V√©rifier s'il y a des donn√©es
            staging_count = df_staging.count()
            
            if staging_count == 0:
                print(f"‚ö†Ô∏è  Aucune donn√©e dans {staging_table}")
                print(f"   (Auto Loader n'a trouv√© aucun nouveau fichier)")
                continue
            
            print(f"‚úÖ {staging_count:,} ligne(s) trouv√©e(s) dans staging")
            
        except Exception as e:
            print(f"‚ùå Table staging introuvable : {e}")
            print(f"   V√©rifier que le Module 2 (Auto Loader) a √©t√© ex√©cut√©")
            
            logger_manager.log_execution(
                source_table, "N/A", "staging", ingestion_mode,
                output_zone, error_msg=f"Staging table not found: {e}",
                status="FAILED", start_time=start_table_time
            )
            total_failed += 1
            continue

        # ========== R√âCUP√âRER LES FICHIERS SOURCES ==========

        # R√©cup√©rer les fichiers uniques trait√©s par Auto Loader
        if "FILE_NAME_RECEIVED" in df_staging.columns:
            source_files = [
                row.FILE_NAME_RECEIVED 
                for row in df_staging.select("FILE_NAME_RECEIVED").distinct().collect()
            ]
            print(f"üìÅ {len(source_files)} fichier(s) source d√©tect√©(s) : {', '.join(source_files[:3])}")
            if len(source_files) > 3:
                print(f"   ... et {len(source_files) - 3} autre(s)")
        else:
            source_files = ["AUTO_LOADER_BATCH"]
            print("‚ö†Ô∏è  Colonne FILE_NAME_RECEIVED manquante - traitement en lot")

        # ‚ùå SUPPRIM√â : Recherche et validation des fichiers
        # Cette logique est maintenant dans autoloader_module.py (Module 2)
        # Les fichiers ont d√©j√† √©t√© valid√©s et lus par Auto Loader

        # ========== CONFIGURATION COLONNES (conserv√©e) ==========

        # Colonnes attendues
        expected_cols = file_columns_df[
            file_columns_df["Delta Table Name"] == source_table
        ]["Column Name"].tolist()

        # Colonnes d√©finitions
        column_defs_for_table = file_columns_df[
            file_columns_df["Delta Table Name"] == source_table
        ].sort_values(by=["Field Order"]) if "Field Order" in file_columns_df.columns else None

        # Colonnes sp√©ciales (merge keys)
        specials = file_columns_df[
            file_columns_df["Delta Table Name"] == source_table
        ].copy()
        
        if "Is Special" in specials.columns:
            specials["Is Special lower"] = specials["Is Special"].astype(str).str.lower()
            merge_keys = specials[specials["Is Special lower"] == "ismergekey"]["Column Name"].tolist()
        else:
            merge_keys = []

        # ========== TRAITEMENT DE CHAQUE FICHIER S√âPAR√âMENT ==========

        print(f"\nüîÑ Traitement s√©par√© de {len(source_files)} fichier(s)")

        for file_idx, filename_current in enumerate(source_files, 1):
            
            print(f"\n{'‚îÄ' * 80}")
            print(f"üìÑ Fichier {file_idx}/{len(source_files)}: {filename_current}")
            print(f"{'‚îÄ' * 80}")

            start_file_time = time.time()

            # ========== ‚ú® NOUVEAU : FILTRER LES DONN√âES DE CE FICHIER ==========

            if "FILE_NAME_RECEIVED" in df_staging.columns and len(source_files) > 1:
                df_raw = df_staging.filter(F.col("FILE_NAME_RECEIVED") == filename_current)
                print(f"   Filtrage sur FILE_NAME_RECEIVED = {filename_current}")
            else:
                df_raw = df_staging  # Traiter toutes les donn√©es en lot
            
            total_rows_initial = df_raw.count()
            print(f"‚úÖ {total_rows_initial:,} ligne(s) pour ce fichier")

            # ‚ùå SUPPRIM√â : file_processor.read_file()
            # Les donn√©es sont d√©j√† lues par Auto Loader
            # df_raw contient maintenant les donn√©es depuis staging

            # ========== TOUTE VOTRE LOGIQUE M√âTIER (100% CONSERV√âE) ==========

            # R√©cup√©rer tol√©rance
            rej_tol_str = trow.get("Rejected line per file tolerance", "10%")
            rej_tol = parse_tolerance(rej_tol_str, total_rows_initial)

            # ‚úÖ VALIDATION COLONNES (conserv√©e)
            is_valid, err_df = validator.validate_columns_presence(
                df_raw, expected_cols, False, source_table, filename_current
            )

            if not is_valid:
                logger_manager.write_quality_errors(err_df, source_table, zone=output_zone)
                logger_manager.log_execution(
                    source_table, filename_current, "staging",
                    ingestion_mode, output_zone,
                    error_msg="Missing columns", status="FAILED",
                    start_time=start_file_time
                )
                total_failed += 1
                continue

            # Ajouter colonnes manquantes (si besoin)
            if expected_cols:
                from pyspark.sql.types import StringType
                for c in expected_cols:
                    if c not in df_raw.columns:
                        df_raw = df_raw.withColumn(c, F.lit(None).cast(StringType()))
                    else:
                        df_raw = df_raw.withColumn(c, df_raw[c].cast(StringType()))

            # ‚úÖ CHECK CORRUPT RECORDS (conserv√©e - file_processor)
            df_raw, corrupt_rows, should_abort = file_processor.check_corrupt_records(
                df_raw, total_rows_initial, rej_tol, source_table, filename_current
            )

            if should_abort:
                logger_manager.log_execution(
                    source_table, filename_current, "staging",
                    ingestion_mode, output_zone,
                    row_count=total_rows_initial, column_count=len(df_raw.columns),
                    error_msg="Too many corrupted lines", status="FAILED",
                    start_time=start_file_time
                )
                total_failed += 1
                continue

            # ‚úÖ TRIM (conserv√©e)
            if trim_flag:
                for c in df_raw.columns:
                    if c not in ["FILE_NAME_RECEIVED", "INGESTION_TIMESTAMP", "yyyy", "mm", "dd"]:
                        df_raw = df_raw.withColumn(c, F.trim(F.col(c)))

            # ‚úÖ TYPAGE COLONNES (conserv√©e - column_processor)
            df_raw, col_errors, invalid_flags = column_processor.process_columns(
                df_raw, column_defs_for_table, table_name=source_table,
                filename=filename_current, total_rows=total_rows_initial
            )

            # ‚úÖ REJETER LIGNES INVALIDES (conserv√©e)
            df_raw, line_errors = column_processor.reject_invalid_lines(
                df_raw, invalid_flags, table_name=source_table, filename=filename_current
            )

            # Fusionner erreurs colonnes
            all_column_errors = []
            if col_errors:
                all_column_errors.extend(col_errors)
            if line_errors:
                all_column_errors.extend(line_errors)

            # ‚úÖ VALIDATION QUALIT√â (conserv√©e - validator)
            df_err_global = validator.check_data_quality(
                df_raw, source_table, merge_keys, filename=filename_current,
                column_defs=file_columns_df
            )

            # ‚úÖ VALIDATION FINALE TYPES (conserv√©e - validator)
            df_raw, errors_list, column_errors = validator.validate_and_rebuild_dataframe(
                df_raw, column_defs_for_table, source_table, filename_current
            )

            # Fusionner toutes erreurs
            if all_column_errors:
                df_col_err = reduce(lambda a, b: a.union(b), all_column_errors)
                df_err_global = df_err_global.union(df_col_err) if df_err_global else df_col_err

            if errors_list:
                df_final_err = reduce(lambda a, b: a.union(b), errors_list)
                df_err_global = df_err_global.union(df_final_err) if df_err_global else df_final_err

            # ‚úÖ INGESTION (conserv√©e - ingestion_manager)
            print(f"\nüíæ Ingestion fichier : {filename_current}")
            print(f"   Mode: {ingestion_mode}")
            print(f"   Destination : {config.catalog}.{config.schema_tables}.{source_table}_{{all,last}}")

            # Extraire date depuis colonnes yyyy, mm, dd (d√©j√† ajout√©es par Auto Loader)
            parts = {}
            if "yyyy" in df_raw.columns:
                yyyy_val = df_raw.select("yyyy").first()
                if yyyy_val and yyyy_val.yyyy:
                    parts["yyyy"] = yyyy_val.yyyy
            if "mm" in df_raw.columns:
                mm_val = df_raw.select("mm").first()
                if mm_val and mm_val.mm:
                    parts["mm"] = mm_val.mm
            if "dd" in df_raw.columns:
                dd_val = df_raw.select("dd").first()
                if dd_val and dd_val.dd:
                    parts["dd"] = dd_val.dd

            try:
                ingestion_manager.apply_ingestion_mode(
                    df_raw, column_defs=column_defs_for_table,
                    table_name=source_table,
                    ingestion_mode=ingestion_mode, zone=output_zone,
                    parts=parts,
                    file_name_received=filename_current
                )
            except Exception as e:
                print(f"‚ùå Erreur ingestion : {e}")
                import traceback
                traceback.print_exc()
                logger_manager.log_execution(
                    source_table, filename_current, "staging",
                    ingestion_mode, output_zone,
                    row_count=0, column_count=len(df_raw.columns),
                    error_msg=f"Ingestion error: {e}",
                    status="FAILED", start_time=start_file_time
                )
                total_failed += 1
                continue

            # ‚úÖ M√âTRIQUES & LOGS (conserv√©s - logger_manager)
            metrics = logger_manager.calculate_final_metrics(df_raw, df_err_global)

            logger_manager.print_summary(
                table_name=source_table, filename=filename_current,
                total_rows=(total_rows_initial, metrics["total_rows_after"]),
                corrupt_rows=metrics["corrupt_rows"],
                anomalies_total=metrics["anomalies_total"],
                cleaned_rows=metrics["cleaned_rows"], errors_df=df_err_global
            )

            logger_manager.write_quality_errors(df_err_global, source_table, zone=output_zone)

            logger_manager.log_execution(
                table_name=source_table, filename=filename_current,
                input_format="staging",  # ‚ú® Chang√© : "staging" au lieu de "csv"
                ingestion_mode=ingestion_mode, output_zone=output_zone,
                row_count=metrics["total_rows_after"],
                column_count=len(df_raw.columns),
                masking_applied=(output_zone == "secret"),
                error_msg=f"{metrics['anomalies_total']} errors" if metrics["anomalies_total"] > 0 else None,
                status="SUCCESS", error_count=metrics["anomalies_total"],
                start_time=start_file_time
            )

            duration = round(time.time() - start_file_time, 2)
            print(f"\n‚úÖ Fichier {filename_current} trait√© en {duration}s")
            total_success += 1
            total_files_processed += 1

        # ========== VIDER TABLE STAGING APR√àS TRAITEMENT ==========
        
        print(f"\nüóëÔ∏è  Nettoyage table staging : {staging_table}")
        try:
            spark.sql(f"DELETE FROM {staging_table}")
            print(f"‚úÖ Table staging vid√©e")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur nettoyage staging : {e}")

    # ========== R√âSUM√â FINAL (conserv√©) ==========

    execution_time = time.time() - start_total_time
    
    print("\n" + "=" * 80)
    print("üéâ TRAITEMENT TERMIN√â")
    print("=" * 80)
    print(f"‚úÖ Fichiers trait√©s avec succ√®s : {total_files_processed}")
    print(f"‚ùå Fichiers en √©chec : {total_failed}")
    print("=" * 80)

    # ========== RAPPORT SIMPLIFI√â (conserv√©) ==========

    if total_files_processed > 0 or total_failed > 0:
        try:
            report_manager = SimpleReportManager(spark, config)
            report_manager.generate_simple_report(
                total_files_processed=total_files_processed,
                total_failed=total_failed,
                execution_time=execution_time
            )
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur g√©n√©ration rapport : {e}")
            import traceback
            traceback.print_exc()

    # ========== DASHBOARDS (optionnel - conserv√©) ==========

    if total_files_processed > 0:
        try:
            print("\nüìä Dashboards disponibles via dashboard_manager.display_all_dashboards()")
            # D√©commenter si vous voulez afficher les dashboards automatiquement :
            # dashboard_manager.display_all_dashboards()
        except Exception as e:
            print(f"‚ö†Ô∏è Info dashboards : {e}")

    print("\nüéØ Pipeline termin√© !")


if __name__ == "__main__":
    main()
```

---

## üìä **R√©sum√© des Changements dans main.py**

| Section | Avant (Ligne) | Statut | Apr√®s |
|---------|--------------|--------|-------|
| **Imports** | 1-28 | ‚úÖ Identique | Aucun changement |
| **Configuration** | 38-74 | ‚úÖ Identique | Aucun changement |
| **Spark Init** | 76-79 | ‚úÖ Identique | Aucun changement |
| **Managers** | 81-94 | ‚úÖ Identique | file_processor simplifi√© mais toujours l√† |
| **Logs Init** | 96-104 | ‚úÖ Identique | Aucun changement |
| **Extraction ZIP** | 106-115 | ‚ùå **SUPPRIM√â** | ‚Üí unzip_module.py (Module 1) |
| **Lecture Excel** | 117-134 | ‚úÖ Modifi√© | Chemin chang√© : `input/config/` |
| **Recherche fichiers** | 194-217 | ‚ùå **SUPPRIM√â** | ‚Üí Auto Loader fait √ßa |
| **Validation fichiers** | 221-283 | ‚ùå **SUPPRIM√â** | ‚Üí Auto Loader fait √ßa |
| **read_file()** | 360-372 | ‚ùå **SUPPRIM√â** | Remplac√© par lecture staging |
| **Lecture staging** | - | ‚úÖ **NOUVEAU** | `spark.table(staging_table)` |
| **Validations** | 374-450 | ‚úÖ Identique | 100% conserv√© |
| **Typage** | 452-490 | ‚úÖ Identique | 100% conserv√© |
| **Qualit√©** | 492-510 | ‚úÖ Identique | 100% conserv√© |
| **Ingestion** | 512-545 | ‚úÖ Identique | 100% conserv√© |
| **Logging** | 547-580 | ‚úÖ Identique | 100% conserv√© |
| **Rapport** | 582-620 | ‚úÖ Identique | 100% conserv√© |

---

## üìà **Statistiques de Code**
```
ANCIEN main.py : ~620 lignes

Supprim√© :
  - extract_zip()           : ~10 lignes
  - Recherche fichiers      : ~25 lignes  
  - Validation fichiers     : ~60 lignes
  - read_file()             : ~15 lignes
                              --------
  Total supprim√©            : ~110 lignes

Ajout√© :
  - Lecture staging         : ~30 lignes
  - Commentaires            : ~20 lignes
                              --------
  Total ajout√©              : ~50 lignes

NOUVEAU main.py : ~560 lignes (-60 lignes)

Code conserv√© : 92%
