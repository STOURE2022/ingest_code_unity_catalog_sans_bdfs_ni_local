"""
main.py - MODULE 3 : WAX PIPELINE INGESTION
Traitement final : Validation, Transformation et Ingestion
Lit depuis les tables _staging crÃ©Ã©es par Auto Loader (Module 2)
"""

import sys
import os
import time
from functools import reduce

# Import modules WAX (TOUS conservÃ©s !)
from config import Config
from validator import DataValidator
from file_processor import FileProcessor  # âœ… SimplifiÃ© mais conservÃ©
from column_processor import ColumnProcessor
from delta_manager import DeltaManager
from logger_manager import LoggerManager
from ingestion import IngestionManager
from dashboards import DashboardManager
from maintenance import MaintenanceManager
from simple_report_manager import SimpleReportManager
from utils import (
    parse_bool, parse_tolerance, safe_count
)

import pandas as pd
from pyspark.sql import SparkSession, functions as F


def main():
    """Point d'entrÃ©e Module 3 - Traitement des donnÃ©es staging"""

    # Timer global pour le rapport
    start_total_time = time.time()

    print("=" * 80)
    print("ğŸš€ WAX PIPELINE - MODULE 3 : INGESTION")
    print("   (Lecture depuis tables staging)")
    print("=" * 80)

    # ========== CONFIGURATION ==========

    print("\nâš™ï¸  Configuration Unity Catalog...")

    config = Config(
        catalog="abu_catalog",
        schema_files="databricksassetbundletest",
        volume="externalvolumetes",
        schema_tables="gdp_poc_dev",
        env="dev",
        version="v1",
        auto_detect_files=False  # âŒ Plus utilisÃ© (c'Ã©tait pour le ZIP)
    )

    config.print_config()

    # Validation
    validation = config.validate_paths()
    if not validation["valid"]:
        print("\nâš ï¸ Avertissements:")
        for issue in validation["issues"]:
            print(f"  {issue}")
    else:
        print(f"\nâœ… Configuration validÃ©e - {validation['mode']}")
        for issue in validation["issues"]:
            if issue.startswith("â„¹ï¸"):
                print(f"  {issue}")

    # ========== INITIALISATION SPARK ==========

    print("\nâš¡ Initialisation Spark...")
    spark = SparkSession.builder.getOrCreate()
    print(f"âœ… Spark version : {spark.version}")

    # ========== MANAGERS (TOUS CONSERVÃ‰S !) ==========

    validator = DataValidator(spark)
    file_processor = FileProcessor(spark, config)  # âœ… Version simplifiÃ©e
    column_processor = ColumnProcessor(spark, config)
    delta_manager = DeltaManager(spark, config)
    logger_manager = LoggerManager(spark, config)
    ingestion_manager = IngestionManager(spark, config, delta_manager)
    dashboard_manager = DashboardManager(spark, config)
    maintenance_manager = MaintenanceManager(spark, config)

    print("âœ… Managers initialisÃ©s\n")

    # ========== INITIALISATION TABLES DE LOGS ==========

    print("ğŸ”§ Initialisation tables de logs...")
    try:
        dashboard_manager.create_execution_logs_table()
        dashboard_manager.create_quality_logs_table()
        print("âœ… Tables de logs prÃªtes\n")
    except Exception as e:
        print(f"âš ï¸ Avertissement tables logs : {e}\n")

    # âŒ SUPPRIMÃ‰ : EXTRACTION ZIP
    # Cette section est maintenant dans unzip_module.py (Module 1)
    # Les fichiers ont dÃ©jÃ  Ã©tÃ© extraits par le Module 1

    # ========== LECTURE EXCEL ==========

    print("=" * 80)
    print("ğŸ“‘ LECTURE CONFIGURATION EXCEL")
    print("=" * 80)

    # âœ¨ MODIFIÃ‰ : Chemin Excel depuis input/config/
    excel_path = f"{config.volume_base}/input/config/wax_configuration.xlsx"
    print(f"ğŸ“„ Fichier : {excel_path}")

    try:
        file_columns_df = pd.read_excel(
            excel_path.replace("/Volumes", "/dbfs/Volumes"),
            sheet_name="Field-Column"
        )
        file_tables_df = pd.read_excel(
            excel_path.replace("/Volumes", "/dbfs/Volumes"),
            sheet_name="File-Table"
        )
        print(f"âœ… Config chargÃ©e : {len(file_tables_df)} tables, {len(file_columns_df)} colonnes\n")
    except Exception as e:
        print(f"âŒ Erreur lecture Excel : {e}")
        import traceback
        traceback.print_exc()
        return

    # ========== TRAITEMENT TABLES STAGING ==========

    print("=" * 80)
    print("ğŸ“Š TRAITEMENT DES TABLES STAGING")
    print("=" * 80)

    total_success = 0
    total_failed = 0
    total_files_processed = 0

    for table_idx, trow in file_tables_df.iterrows():
        start_table_time = time.time()

        source_table = trow["Delta Table Name"]
        output_zone = str(trow.get("Output Zone", "internal")).strip().lower()
        ingestion_mode = str(trow.get("Ingestion mode", "")).strip()

        print(f"\n{'=' * 80}")
        print(f"ğŸ“‹ Table {table_idx + 1}/{len(file_tables_df)}: {source_table}")
        print(f"   Zone: {output_zone}, Mode: {ingestion_mode}")
        print(f"{'=' * 80}")

        # Options (conservÃ©es pour les transformations)
        trim_flag = parse_bool(trow.get("Trim", True), True)

        # ========== âœ¨ NOUVEAU : LECTURE DEPUIS TABLE STAGING ==========

        staging_table = f"{config.catalog}.{config.schema_tables}.{source_table}_staging"
        
        print(f"\nğŸ“– Lecture depuis table staging : {staging_table}")

        try:
            df_staging = spark.table(staging_table)
            
            # VÃ©rifier s'il y a des donnÃ©es
            staging_count = df_staging.count()
            
            if staging_count == 0:
                print(f"âš ï¸  Aucune donnÃ©e dans {staging_table}")
                print(f"   (Auto Loader n'a trouvÃ© aucun nouveau fichier)")
                continue
            
            print(f"âœ… {staging_count:,} ligne(s) trouvÃ©e(s) dans staging")
            
        except Exception as e:
            print(f"âŒ Table staging introuvable : {e}")
            print(f"   VÃ©rifier que le Module 2 (Auto Loader) a Ã©tÃ© exÃ©cutÃ©")
            
            logger_manager.log_execution(
                source_table, "N/A", "staging", ingestion_mode,
                output_zone, error_msg=f"Staging table not found: {e}",
                status="FAILED", start_time=start_table_time
            )
            total_failed += 1
            continue

        # ========== RÃ‰CUPÃ‰RER LES FICHIERS SOURCES ==========

        # RÃ©cupÃ©rer les fichiers uniques traitÃ©s par Auto Loader
        if "FILE_NAME_RECEIVED" in df_staging.columns:
            source_files = [
                row.FILE_NAME_RECEIVED 
                for row in df_staging.select("FILE_NAME_RECEIVED").distinct().collect()
            ]
            print(f"ğŸ“ {len(source_files)} fichier(s) source dÃ©tectÃ©(s) : {', '.join(source_files[:3])}")
            if len(source_files) > 3:
                print(f"   ... et {len(source_files) - 3} autre(s)")
        else:
            source_files = ["AUTO_LOADER_BATCH"]
            print("âš ï¸  Colonne FILE_NAME_RECEIVED manquante - traitement en lot")

        # âŒ SUPPRIMÃ‰ : Recherche et validation des fichiers
        # Cette logique est maintenant dans autoloader_module.py (Module 2)
        # Les fichiers ont dÃ©jÃ  Ã©tÃ© validÃ©s et lus par Auto Loader

        # ========== CONFIGURATION COLONNES (conservÃ©e) ==========

        # Colonnes attendues
        expected_cols = file_columns_df[
            file_columns_df["Delta Table Name"] == source_table
        ]["Column Name"].tolist()

        # Colonnes dÃ©finitions
        column_defs_for_table = file_columns_df[
            file_columns_df["Delta Table Name"] == source_table
        ].sort_values(by=["Field Order"]) if "Field Order" in file_columns_df.columns else None

        # Colonnes spÃ©ciales (merge keys)
        specials = file_columns_df[
            file_columns_df["Delta Table Name"] == source_table
        ].copy()
        
        if "Is Special" in specials.columns:
            specials["Is Special lower"] = specials["Is Special"].astype(str).str.lower()
            merge_keys = specials[specials["Is Special lower"] == "ismergekey"]["Column Name"].tolist()
        else:
            merge_keys = []

        # ========== TRAITEMENT DE CHAQUE FICHIER SÃ‰PARÃ‰MENT ==========

        print(f"\nğŸ”„ Traitement sÃ©parÃ© de {len(source_files)} fichier(s)")

        for file_idx, filename_current in enumerate(source_files, 1):
            
            print(f"\n{'â”€' * 80}")
            print(f"ğŸ“„ Fichier {file_idx}/{len(source_files)}: {filename_current}")
            print(f"{'â”€' * 80}")

            start_file_time = time.time()

            # ========== âœ¨ NOUVEAU : FILTRER LES DONNÃ‰ES DE CE FICHIER ==========

            if "FILE_NAME_RECEIVED" in df_staging.columns and len(source_files) > 1:
                df_raw = df_staging.filter(F.col("FILE_NAME_RECEIVED") == filename_current)
                print(f"   Filtrage sur FILE_NAME_RECEIVED = {filename_current}")
            else:
                df_raw = df_staging  # Traiter toutes les donnÃ©es en lot
            
            total_rows_initial = df_raw.count()
            print(f"âœ… {total_rows_initial:,} ligne(s) pour ce fichier")

            # âŒ SUPPRIMÃ‰ : file_processor.read_file()
            # Les donnÃ©es sont dÃ©jÃ  lues par Auto Loader
            # df_raw contient maintenant les donnÃ©es depuis staging

            # ========== TOUTE VOTRE LOGIQUE MÃ‰TIER (100% CONSERVÃ‰E) ==========

            # RÃ©cupÃ©rer tolÃ©rance
            rej_tol_str = trow.get("Rejected line per file tolerance", "10%")
            rej_tol = parse_tolerance(rej_tol_str, total_rows_initial)

            # âœ… VALIDATION COLONNES (conservÃ©e)
            is_valid, err_df = validator.validate_columns_presence(
                df_raw, expected_cols, False, source_table, filename_current
            )

            if not is_valid:
                logger_manager.write_quality_errors(err_df, source_table, zone=output_zone)
                logger_manager.log_execution(
                    source_table, filename_current, "staging",
                    ingestion_mode, output_zone,
                    error_msg="Missing columns", status="FAILED",
                    start_time=start_file_time
                )
                total_failed += 1
                continue

            # Ajouter colonnes manquantes (si besoin)
            if expected_cols:
                from pyspark.sql.types import StringType
                for c in expected_cols:
                    if c not in df_raw.columns:
                        df_raw = df_raw.withColumn(c, F.lit(None).cast(StringType()))
                    else:
                        df_raw = df_raw.withColumn(c, df_raw[c].cast(StringType()))

            # âœ… CHECK CORRUPT RECORDS (conservÃ©e - file_processor)
            df_raw, corrupt_rows, should_abort = file_processor.check_corrupt_records(
                df_raw, total_rows_initial, rej_tol, source_table, filename_current
            )

            if should_abort:
                logger_manager.log_execution(
                    source_table, filename_current, "staging",
                    ingestion_mode, output_zone,
                    row_count=total_rows_initial, column_count=len(df_raw.columns),
                    error_msg="Too many corrupted lines", status="FAILED",
                    start_time=start_file_time
                )
                total_failed += 1
                continue

            # âœ… TRIM (conservÃ©e)
            if trim_flag:
                for c in df_raw.columns:
                    if c not in ["FILE_NAME_RECEIVED", "INGESTION_TIMESTAMP", "yyyy", "mm", "dd"]:
                        df_raw = df_raw.withColumn(c, F.trim(F.col(c)))

            # âœ… TYPAGE COLONNES (conservÃ©e - column_processor)
            df_raw, col_errors, invalid_flags = column_processor.process_columns(
                df_raw, column_defs_for_table, table_name=source_table,
                filename=filename_current, total_rows=total_rows_initial
            )

            # âœ… REJETER LIGNES INVALIDES (conservÃ©e)
            df_raw, line_errors = column_processor.reject_invalid_lines(
                df_raw, invalid_flags, table_name=source_table, filename=filename_current
            )

            # Fusionner erreurs colonnes
            all_column_errors = []
            if col_errors:
                all_column_errors.extend(col_errors)
            if line_errors:
                all_column_errors.extend(line_errors)

            # âœ… VALIDATION QUALITÃ‰ (conservÃ©e - validator)
            df_err_global = validator.check_data_quality(
                df_raw, source_table, merge_keys, filename=filename_current,
                column_defs=file_columns_df
            )

            # âœ… VALIDATION FINALE TYPES (conservÃ©e - validator)
            df_raw, errors_list, column_errors = validator.validate_and_rebuild_dataframe(
                df_raw, column_defs_for_table, source_table, filename_current
            )

            # Fusionner toutes erreurs
            if all_column_errors:
                df_col_err = reduce(lambda a, b: a.union(b), all_column_errors)
                df_err_global = df_err_global.union(df_col_err) if df_err_global else df_col_err

            if errors_list:
                df_final_err = reduce(lambda a, b: a.union(b), errors_list)
                df_err_global = df_err_global.union(df_final_err) if df_err_global else df_final_err

            # âœ… INGESTION (conservÃ©e - ingestion_manager)
            print(f"\nğŸ’¾ Ingestion fichier : {filename_current}")
            print(f"   Mode: {ingestion_mode}")
            print(f"   Destination : {config.catalog}.{config.schema_tables}.{source_table}_{{all,last}}")

            # Extraire date depuis colonnes yyyy, mm, dd (dÃ©jÃ  ajoutÃ©es par Auto Loader)
            parts = {}
            if "yyyy" in df_raw.columns:
                yyyy_val = df_raw.select("yyyy").first()
                if yyyy_val and yyyy_val.yyyy:
                    parts["yyyy"] = yyyy_val.yyyy
            if "mm" in df_raw.columns:
                mm_val = df_raw.select("mm").first()
                if mm_val and mm_val.mm:
                    parts["mm"] = mm_val.mm
            if "dd" in df_raw.columns:
                dd_val = df_raw.select("dd").first()
                if dd_val and dd_val.dd:
                    parts["dd"] = dd_val.dd

            try:
                ingestion_manager.apply_ingestion_mode(
                    df_raw, column_defs=column_defs_for_table,
                    table_name=source_table,
                    ingestion_mode=ingestion_mode, zone=output_zone,
                    parts=parts,
                    file_name_received=filename_current
                )
            except Exception as e:
                print(f"âŒ Erreur ingestion : {e}")
                import traceback
                traceback.print_exc()
                logger_manager.log_execution(
                    source_table, filename_current, "staging",
                    ingestion_mode, output_zone,
                    row_count=0, column_count=len(df_raw.columns),
                    error_msg=f"Ingestion error: {e}",
                    status="FAILED", start_time=start_file_time
                )
                total_failed += 1
                continue

            # âœ… MÃ‰TRIQUES & LOGS (conservÃ©s - logger_manager)
            metrics = logger_manager.calculate_final_metrics(df_raw, df_err_global)

            logger_manager.print_summary(
                table_name=source_table, filename=filename_current,
                total_rows=(total_rows_initial, metrics["total_rows_after"]),
                corrupt_rows=metrics["corrupt_rows"],
                anomalies_total=metrics["anomalies_total"],
                cleaned_rows=metrics["cleaned_rows"], errors_df=df_err_global
            )

            logger_manager.write_quality_errors(df_err_global, source_table, zone=output_zone)

            logger_manager.log_execution(
                table_name=source_table, filename=filename_current,
                input_format="staging",  # âœ¨ ChangÃ© : "staging" au lieu de "csv"
                ingestion_mode=ingestion_mode, output_zone=output_zone,
                row_count=metrics["total_rows_after"],
                column_count=len(df_raw.columns),
                masking_applied=(output_zone == "secret"),
                error_msg=f"{metrics['anomalies_total']} errors" if metrics["anomalies_total"] > 0 else None,
                status="SUCCESS", error_count=metrics["anomalies_total"],
                start_time=start_file_time
            )

            duration = round(time.time() - start_file_time, 2)
            print(f"\nâœ… Fichier {filename_current} traitÃ© en {duration}s")
            total_success += 1
            total_files_processed += 1

        # ========== VIDER TABLE STAGING APRÃˆS TRAITEMENT ==========
        
        print(f"\nğŸ—‘ï¸  Nettoyage table staging : {staging_table}")
        try:
            spark.sql(f"DELETE FROM {staging_table}")
            print(f"âœ… Table staging vidÃ©e")
        except Exception as e:
            print(f"âš ï¸  Erreur nettoyage staging : {e}")

    # ========== RÃ‰SUMÃ‰ FINAL (conservÃ©) ==========

    execution_time = time.time() - start_total_time
    
    print("\n" + "=" * 80)
    print("ğŸ‰ TRAITEMENT TERMINÃ‰")
    print("=" * 80)
    print(f"âœ… Fichiers traitÃ©s avec succÃ¨s : {total_files_processed}")
    print(f"âŒ Fichiers en Ã©chec : {total_failed}")
    print("=" * 80)

    # ========== RAPPORT SIMPLIFIÃ‰ (conservÃ©) ==========

    if total_files_processed > 0 or total_failed > 0:
        try:
            report_manager = SimpleReportManager(spark, config)
            report_manager.generate_simple_report(
                total_files_processed=total_files_processed,
                total_failed=total_failed,
                execution_time=execution_time
            )
            
        except Exception as e:
            print(f"âš ï¸ Erreur gÃ©nÃ©ration rapport : {e}")
            import traceback
            traceback.print_exc()

    # ========== DASHBOARDS (optionnel - conservÃ©) ==========

    if total_files_processed > 0:
        try:
            print("\nğŸ“Š Dashboards disponibles via dashboard_manager.display_all_dashboards()")
            # DÃ©commenter si vous voulez afficher les dashboards automatiquement :
            # dashboard_manager.display_all_dashboards()
        except Exception as e:
            print(f"âš ï¸ Info dashboards : {e}")

    print("\nğŸ¯ Pipeline terminÃ© !")


if __name__ == "__main__":
    main()
```

---

## ğŸ“Š **RÃ©sumÃ© des Changements dans main.py**

| Section | Avant (Ligne) | Statut | AprÃ¨s |
|---------|--------------|--------|-------|
| **Imports** | 1-28 | âœ… Identique | Aucun changement |
| **Configuration** | 38-74 | âœ… Identique | Aucun changement |
| **Spark Init** | 76-79 | âœ… Identique | Aucun changement |
| **Managers** | 81-94 | âœ… Identique | file_processor simplifiÃ© mais toujours lÃ  |
| **Logs Init** | 96-104 | âœ… Identique | Aucun changement |
| **Extraction ZIP** | 106-115 | âŒ **SUPPRIMÃ‰** | â†’ unzip_module.py (Module 1) |
| **Lecture Excel** | 117-134 | âœ… ModifiÃ© | Chemin changÃ© : `input/config/` |
| **Recherche fichiers** | 194-217 | âŒ **SUPPRIMÃ‰** | â†’ Auto Loader fait Ã§a |
| **Validation fichiers** | 221-283 | âŒ **SUPPRIMÃ‰** | â†’ Auto Loader fait Ã§a |
| **read_file()** | 360-372 | âŒ **SUPPRIMÃ‰** | RemplacÃ© par lecture staging |
| **Lecture staging** | - | âœ… **NOUVEAU** | `spark.table(staging_table)` |
| **Validations** | 374-450 | âœ… Identique | 100% conservÃ© |
| **Typage** | 452-490 | âœ… Identique | 100% conservÃ© |
| **QualitÃ©** | 492-510 | âœ… Identique | 100% conservÃ© |
| **Ingestion** | 512-545 | âœ… Identique | 100% conservÃ© |
| **Logging** | 547-580 | âœ… Identique | 100% conservÃ© |
| **Rapport** | 582-620 | âœ… Identique | 100% conservÃ© |

---

## ğŸ“ˆ **Statistiques de Code**
```
ANCIEN main.py : ~620 lignes

SupprimÃ© :
  - extract_zip()           : ~10 lignes
  - Recherche fichiers      : ~25 lignes  
  - Validation fichiers     : ~60 lignes
  - read_file()             : ~15 lignes
                              --------
  Total supprimÃ©            : ~110 lignes

AjoutÃ© :
  - Lecture staging         : ~30 lignes
  - Commentaires            : ~20 lignes
                              --------
  Total ajoutÃ©              : ~50 lignes

NOUVEAU main.py : ~560 lignes (-60 lignes)

Code conservÃ© : 92%
