ðŸ“ Nouveau autoloader_module.py - Version Content-Based
"""
autoloader_module.py
MODULE 2 : Auto Loader - Content-Based Discovery
DÃ©tecte les tables depuis les noms des fichiers CSV, pas des dossiers
"""

import json
import os
from collections import defaultdict
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *


class AutoLoaderModule:
    """Module Auto Loader avec dÃ©tection basÃ©e sur le contenu des dossiers"""
    
    def __init__(self, spark, config):
        self.spark = spark
        self.config = config
        
        # Chemins Unity Catalog
        self.source_base = f"{config.volume_base}/extracted"
        self.checkpoint_base = f"{config.volume_base}/checkpoints"
        self.schema_base = f"{config.volume_base}/schemas"
    
    def process_all_tables(self, excel_config_path: str) -> dict:
        """
        Lance Auto Loader pour tous les fichiers CSV trouvÃ©s dans extracted/
        DÃ©tecte les tables depuis les noms des fichiers
        
        Args:
            excel_config_path: Chemin du fichier Excel de config
            
        Returns:
            dict: RÃ©sultats du traitement
        """
        
        print("=" * 80)
        print("ðŸ”„ MODULE 2 : AUTO LOADER (CONTENT-BASED DISCOVERY)")
        print("=" * 80)
        
        # Lire configuration Excel
        import pandas as pd
        
        print(f"\nðŸ“– Lecture configuration : {excel_config_path}")
        
        try:
            file_tables_df = pd.read_excel(excel_config_path, sheet_name="File-Table")
            file_columns_df = pd.read_excel(excel_config_path, sheet_name="Field-Column")
            print(f"âœ… Configuration chargÃ©e : {len(file_tables_df)} table(s)\n")
        except Exception as e:
            print(f"âŒ Erreur lecture Excel : {e}")
            import traceback
            traceback.print_exc()
            return {"status": "ERROR", "error": str(e)}
        
        # ========== âœ¨ NOUVEAU : DÃ‰COUVERTE BASÃ‰E SUR LES FICHIERS CSV ==========
        
        print(f"ðŸ” Scan rÃ©cursif du rÃ©pertoire : {self.source_base}")
        
        # DÃ©couvrir tous les fichiers CSV dans extracted/ et sous-dossiers
        files_by_table = self._discover_csv_files(self.source_base)
        
        if not files_by_table:
            print(f"âš ï¸  Aucun fichier CSV trouvÃ© dans extracted/")
            return {"status": "NO_DATA", "message": "No CSV files found"}
        
        print(f"\nâœ… {len(files_by_table)} table(s) dÃ©tectÃ©e(s) depuis les fichiers CSV :")
        for table_name, files_info in files_by_table.items():
            print(f"   â€¢ {table_name}: {len(files_info['files'])} fichier(s)")
        
        # ========== TRAITER CHAQUE TABLE DÃ‰TECTÃ‰E ==========
        
        results = []
        success_count = 0
        failed_count = 0
        total_rows = 0
        
        for idx, (table_name, files_info) in enumerate(files_by_table.items(), 1):
            
            print(f"\n{'=' * 80}")
            print(f"ðŸ“‹ Table {idx}/{len(files_by_table)}: {table_name}")
            print(f"{'=' * 80}")
            
            # Chercher la configuration correspondante dans l'Excel
            table_config = self._find_table_config(table_name, file_tables_df)
            
            if table_config is None:
                print(f"âš ï¸  Aucune configuration trouvÃ©e dans Excel pour '{table_name}'")
                print(f"   â†’ Utilisation de la configuration par dÃ©faut")
                
                # CrÃ©er config par dÃ©faut
                table_config = {
                    "Delta Table Name": table_name,
                    "Input Format": "csv",
                    "Input delimiter": ",",
                    "Input charset": "UTF-8"
                }
            else:
                print(f"âœ… Configuration trouvÃ©e : table '{table_config['Delta Table Name']}'")
            
            print(f"ðŸ“ {len(files_info['files'])} fichier(s) CSV :")
            for file_path in files_info['files'][:5]:  # Afficher les 5 premiers
                print(f"   â€¢ {file_path}")
            if len(files_info['files']) > 5:
                print(f"   ... et {len(files_info['files']) - 5} autre(s)")
            
            # Colonnes de cette table
            if isinstance(table_config, dict):
                config_table_name = table_config["Delta Table Name"]
            else:
                config_table_name = table_config["Delta Table Name"]
            
            table_columns = file_columns_df[
                file_columns_df["Delta Table Name"] == config_table_name
            ]
            
            # Traiter avec Auto Loader
            try:
                result = self._process_single_table(
                    table_name,
                    files_info,
                    table_config, 
                    table_columns
                )
                results.append(result)
                
                if result["status"] == "SUCCESS":
                    print(f"âœ… {result.get('rows_ingested', 0):,} ligne(s) ingÃ©rÃ©e(s)")
                    success_count += 1
                    total_rows += result.get('rows_ingested', 0)
                elif result["status"] == "NO_DATA":
                    print(f"âš ï¸  Aucune nouvelle donnÃ©e")
                else:
                    print(f"âŒ Ã‰chec : {result.get('error', 'Unknown')}")
                    failed_count += 1
                    
            except Exception as e:
                print(f"âŒ Erreur : {e}")
                import traceback
                traceback.print_exc()
                
                failed_count += 1
                results.append({
                    "table": table_name,
                    "status": "ERROR",
                    "error": str(e)
                })
        
        # RÃ©sumÃ©
        print("\n" + "=" * 80)
        print("ðŸ“Š RÃ‰SUMÃ‰ AUTO LOADER")
        print("=" * 80)
        print(f"âœ… Tables traitÃ©es  : {success_count}")
        print(f"âŒ Tables en Ã©chec  : {failed_count}")
        print(f"ðŸ“ˆ Total lignes     : {total_rows:,}")
        print("=" * 80)
        
        return {
            "status": "SUCCESS" if failed_count == 0 else "PARTIAL",
            "success_count": success_count,
            "failed_count": failed_count,
            "total_rows": total_rows,
            "results": results
        }
    
    def _discover_csv_files(self, base_dir: str) -> dict:
        """
        DÃ©couvre tous les fichiers CSV et les groupe par table
        
        Args:
            base_dir: RÃ©pertoire de base Ã  scanner
            
        Returns:
            dict: {table_name: {"files": [paths], "folders": [folders]}}
        """
        
        files_by_table = defaultdict(lambda: {"files": [], "folders": set()})
        
        if not os.path.exists(base_dir):
            return {}
        
        # Scanner rÃ©cursivement
        for root, dirs, files in os.walk(base_dir):
            # Ignorer dossiers cachÃ©s
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                # Ignorer fichiers cachÃ©s et non-CSV
                if file.startswith('.') or not file.lower().endswith('.csv'):
                    continue
                
                # Extraire le nom de la table depuis le nom du fichier
                table_name = self._extract_table_name(file)
                
                if table_name:
                    file_path = os.path.join(root, file)
                    folder_path = root
                    
                    files_by_table[table_name]["files"].append(file_path)
                    files_by_table[table_name]["folders"].add(folder_path)
        
        # Convertir sets en listes pour sÃ©rialisation
        for table_name in files_by_table:
            files_by_table[table_name]["folders"] = list(files_by_table[table_name]["folders"])
        
        return dict(files_by_table)
    
    def _extract_table_name(self, filename: str) -> str:
        """
        Extrait le nom de la table depuis le nom du fichier
        
        Exemples:
            site_20250902_120001.csv â†’ site
            customer_20250902.csv â†’ customer
            product.csv â†’ product
            SITE_20250902.CSV â†’ site
        
        Args:
            filename: Nom du fichier
            
        Returns:
            Nom de la table (lowercase)
        """
        
        # Retirer l'extension
        basename = os.path.splitext(filename)[0]
        
        # StratÃ©gies d'extraction (par ordre de prioritÃ©)
        
        # 1. Pattern standard : table_YYYYMMDD_HHMMSS
        if '_' in basename:
            parts = basename.split('_')
            
            # Chercher la premiÃ¨re partie qui n'est pas une date/heure
            for part in parts:
                # Si c'est pas un nombre (ou pas que des chiffres), c'est probablement le nom
                if not part.isdigit():
                    return part.lower()
            
            # Si tout est numÃ©rique, prendre la premiÃ¨re partie
            return parts[0].lower()
        
        # 2. Pas de underscore : le nom complet est le nom de la table
        return basename.lower()
    
    def _find_table_config(self, table_name: str, file_tables_df) -> dict:
        """
        Cherche la configuration correspondante dans l'Excel
        
        Args:
            table_name: Nom de la table extrait des fichiers
            file_tables_df: DataFrame Excel File-Table
            
        Returns:
            Configuration trouvÃ©e ou None
        """
        
        table_lower = table_name.lower()
        
        for idx, row in file_tables_df.iterrows():
            config_table = str(row["Delta Table Name"]).strip().lower()
            
            # Match exact
            if config_table == table_lower:
                print(f"   ðŸŽ¯ Match exact : '{table_name}' = '{row['Delta Table Name']}'")
                return row
            
            # Match partiel
            if table_lower in config_table or config_table in table_lower:
                print(f"   ðŸŽ¯ Match partiel : '{table_name}' â†” '{row['Delta Table Name']}'")
                return row
        
        # Aucun match
        return None
    
    def _process_single_table(self, table_name: str, files_info: dict, 
                              table_config, columns_config) -> dict:
        """
        Traite une table avec Auto Loader
        
        Args:
            table_name: Nom de la table (extrait des CSV)
            files_info: Info sur les fichiers (paths, folders)
            table_config: Configuration de la table
            columns_config: DÃ©finitions des colonnes
            
        Returns:
            dict: RÃ©sultat du traitement
        """
        
        # DÃ©terminer les dossiers sources
        # Si les fichiers sont rÃ©partis dans plusieurs dossiers, on prend le parent commun
        folders = files_info["folders"]
        
        if len(folders) == 1:
            source_path = folders[0]
        else:
            # Trouver le parent commun
            source_path = os.path.commonpath(folders)
        
        # Chemins (Unity Catalog)
        checkpoint_path = f"{self.checkpoint_base}/{table_name}"
        schema_path = f"{self.schema_base}/{table_name}"
        
        # Configuration table
        if isinstance(table_config, dict):
            config_table_name = table_config["Delta Table Name"]
        else:
            config_table_name = table_config["Delta Table Name"]
        
        target_table = f"{self.config.catalog}.{self.config.schema_tables}.{config_table_name}_staging"
        
        print(f"\nðŸ“‚ Source(s)   : {source_path}")
        print(f"ðŸ“‚ Checkpoint  : {checkpoint_path}")
        print(f"ðŸ—„ï¸  Target      : {target_table}")
        
        # Configuration lecture
        if isinstance(table_config, dict):
            input_format = str(table_config.get("Input Format", "csv")).strip().lower()
            delimiter = str(table_config.get("Input delimiter", ","))
            charset = str(table_config.get("Input charset", "UTF-8")).strip()
        else:
            input_format = str(table_config.get("Input Format", "csv")).strip().lower()
            delimiter = str(table_config.get("Input delimiter", ","))
            charset = str(table_config.get("Input charset", "UTF-8")).strip()
        
        if charset.lower() in ["nan", "", "none"]:
            charset = "UTF-8"
        
        # Options Auto Loader avec pattern matching pour cette table
        options = {
            "cloudFiles.format": input_format,
            "cloudFiles.useNotifications": "false",
            "cloudFiles.includeExistingFiles": "true",
            "cloudFiles.schemaLocation": schema_path,
        }
        
        # Options CSV
        if input_format in ["csv", "csv_quote", "csv_quote_ml"]:
            options.update({
                "header": "true",
                "delimiter": delimiter,
                "encoding": charset,
                "inferSchema": "false",
                "mode": "PERMISSIVE",
                "columnNameOfCorruptRecord": "_corrupt_record",
                "quote": '"',
                "escape": "\\"
            })
            
            if input_format == "csv_quote_ml":
                options["multiline"] = "true"
        
        # CrÃ©er stream
        print(f"\nðŸ”„ CrÃ©ation stream Auto Loader...")
        print(f"   Pattern: {table_name}_*.csv")
        
        try:
            # Lire avec pattern pour ne prendre que les fichiers de cette table
            df_stream = (
                self.spark.readStream
                .format("cloudFiles")
                .options(**options)
                .load(source_path)
            )
            
            # Filtrer par nom de fichier pour ne garder que cette table
            df_stream = df_stream.filter(
                F.element_at(F.split(F.input_file_name(), "/"), -1).startswith(f"{table_name}_")
                | F.element_at(F.split(F.input_file_name(), "/"), -1).rlike(f"^{table_name}\\.csv$")
            )
            
        except Exception as e:
            return {
                "status": "ERROR",
                "error": f"Stream creation failed: {e}"
            }
        
        # Ajouter mÃ©tadonnÃ©es
        df_stream = self._add_metadata(df_stream)
        
        # Ã‰crire dans table staging
        print(f"ðŸ’¾ Ã‰criture vers {target_table}...")
        
        try:
            query = (
                df_stream.writeStream
                .format("delta")
                .outputMode("append")
                .option("checkpointLocation", checkpoint_path)
                .option("mergeSchema", "true")
                .trigger(once=True)
                .toTable(target_table)
            )
            
            print(f"â³ Traitement en cours...")
            query.awaitTermination()
            
            # Statistiques
            progress = query.lastProgress
            
            if progress:
                rows_ingested = progress.get("numInputRows", 0)
                
                if rows_ingested > 0:
                    return {
                        "status": "SUCCESS",
                        "rows_ingested": rows_ingested,
                        "target_table": target_table,
                        "source_files": len(files_info["files"])
                    }
                else:
                    return {
                        "status": "NO_DATA",
                        "rows_ingested": 0,
                        "target_table": target_table
                    }
            else:
                return {
                    "status": "NO_DATA",
                    "rows_ingested": 0,
                    "target_table": target_table
                }
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return {
                "status": "ERROR",
                "error": f"Write failed: {e}"
            }
    
    def _add_metadata(self, df_stream):
        """Ajoute mÃ©tadonnÃ©es au stream"""
        
        # Nom fichier
        df_stream = df_stream.withColumn(
            "FILE_NAME_RECEIVED",
            F.element_at(F.split(F.input_file_name(), "/"), -1)
        )
        
        # Date depuis nom fichier
        df_stream = df_stream.withColumn(
            "yyyy",
            F.regexp_extract(F.col("FILE_NAME_RECEIVED"), r"_(\d{4})\d{4}", 1).cast("int")
        )
        
        df_stream = df_stream.withColumn(
            "mm",
            F.regexp_extract(F.col("FILE_NAME_RECEIVED"), r"_\d{4}(\d{2})\d{2}", 1).cast("int")
        )
        
        df_stream = df_stream.withColumn(
            "dd",
            F.regexp_extract(F.col("FILE_NAME_RECEIVED"), r"_\d{6}(\d{2})", 1).cast("int")
        )
        
        # Timestamp ingestion
        df_stream = df_stream.withColumn(
            "INGESTION_TIMESTAMP",
            F.current_timestamp()
        )
        
        return df_stream
    
    def list_staging_tables(self) -> list:
        """Liste les tables staging crÃ©Ã©es"""
        
        try:
            tables = self.spark.sql(
                f"SHOW TABLES IN {self.config.catalog}.{self.config.schema_tables}"
            ).collect()
            
            staging_tables = [t.tableName for t in tables if "_staging" in t.tableName]
            
            return staging_tables
            
        except Exception as e:
            print(f"âš ï¸  Erreur listage tables staging : {e}")
            return []
    
    def get_staging_stats(self) -> dict:
        """RÃ©cupÃ¨re les statistiques des tables staging"""
        
        staging_tables = self.list_staging_tables()
        
        stats = {}
        
        for table_name in staging_tables:
            table_full = f"{self.config.catalog}.{self.config.schema_tables}.{table_name}"
            
            try:
                df = self.spark.table(table_full)
                count = df.count()
                
                # Fichiers sources
                sources = []
                if "FILE_NAME_RECEIVED" in df.columns:
                    sources = [row.FILE_NAME_RECEIVED 
                             for row in df.select("FILE_NAME_RECEIVED").distinct().collect()]
                
                stats[table_name] = {
                    "rows": count,
                    "sources": sources
                }
                
            except Exception as e:
                stats[table_name] = {
                    "error": str(e)
                }
        
        return stats


def main():
    """Point d'entrÃ©e du module Auto Loader"""
    
    import sys
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    
    from config import Config
    
    print("ðŸš€ DÃ©marrage Module 2 : Auto Loader (Content-Based)")
    
    # Initialiser Spark
    spark = SparkSession.builder.appName("WAX-Module2-AutoLoader").getOrCreate()
    
    # Configuration
    config = Config(
        catalog="abu_catalog",
        schema_files="databricksassetbundletest",
        volume="externalvolumetes",
        schema_tables="gdp_poc_dev",
        env="dev",
        version="v1"
    )
    
    # Chemin Excel
    excel_path = f"{config.volume_base}/input/config/wax_config.xlsx"
    
    # Auto Loader
    autoloader = AutoLoaderModule(spark, config)
    result = autoloader.process_all_tables(excel_path)
    
    # Afficher tables staging crÃ©Ã©es
    if result["status"] in ["SUCCESS", "PARTIAL"]:
        print("\nðŸ“‹ Tables staging crÃ©Ã©es :")
        stats = autoloader.get_staging_stats()
        
        for table_name, table_stats in stats.items():
            if "error" not in table_stats:
                print(f"   â€¢ {table_name}: {table_stats['rows']:,} lignes")
                if table_stats.get('sources'):
                    print(f"     Sources: {', '.jo
